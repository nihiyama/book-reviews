# Kubernetes Best Practicesを読んで

## Chapter 1. Setting Up a Basic Service

- セキュリティ対策として公式で用意されているイメージ使え。なるべく小さいイメージ使え（scratchとか）
- コンテナイメージはセマンティックバージョンとSHAハッシュつけろ（例えばv1.0.1-dfha92）
- podのresouceとrequestは合わせておくと予測しやすくて便利
  - ただしリソースは犠牲になる
  - 運用しやすさを取るかリソース消費を取るか
- クラスタの内容とソースが一致しているようにしろ→GitOpsが最善の方法
- 設定情報をアプリケーション自体から分離する→ConfigMap
- ConfigMap自体を変更することでロールアウトしたくなるかもしれないが、これは実際にはベストプラクティスではない
  - より良いアプローチは、ConfigMap自体の名前にバージョン番号を入れることです。変更を加える場合、ConfigMapを更新するのではなく、新しいv2 ConfigMapを作成し、その構成を使用するようにDeploymentリソースを更新する
  - このとき、適切なヘルスチェックと変更間のポーズを使用して、デプロイのロールアウトが自動的にトリガされる。さらに、ロールバックが必要な場合は、v1構成がクラスタに格納されているので、ロールバックはDeploymentを再度更新するだけでよい。
- パスワードはソースコードやコンテイメージに入れるな
- Kubernetesのシークレットは、デフォルトでは暗号化されていない状態で保存される。秘密を暗号化して保存したい場合は、鍵プロバイダと統合して、Kubernetesがクラスタ内のすべての秘密を暗号化するために使用する鍵を提供することができる。これにより、etcdデータベースへの直接攻撃に対する鍵のセキュリティは確保されるが、Kubernetes APIサーバーを介したアクセスが適切に保護されていることを確認する必要があることに注意する。
  - secretの場合、Volumeはtmpfs RAM-backed filesystemとして作成され、その後コンテナにマウントされる。これにより、マシンが物理的に侵害されたとしても（クラウドでは可能性が極めて低いが、データセンターでは可能）、攻撃者が秘密を入手することがはるかに困難になる。
- ステートフルなワークロードを実行する場合、リモートPersistentVolumesを使用して、アプリケーションに関連する状態を管理することが重要
  - ステートフルなサービスはSaaSを使うのが良いとされている
  - ステートのすべての運用要件（バックアップ、データの局所性、冗長性など）や、Kubernetesクラスタにステートがあるとクラスタ間でアプリケーションを移動するのが難しくなるという事実を考慮すると、ほとんどの場合、ストレージSaaSは価格プレミアムに値することは明らか
- 多くのチームにとって最初の失敗モードは、単にファイルをあるクラスタから別のクラスタにコピーしてしまうこと。単一の frontend/ ディレクトリを持つ代わりに、frontend-production/ と frontend-development/ というディレクトリのペアを持つようにする。
- その結果、ほとんどの人はテンプレート・システムを使う->Helm
- ほとんどのサービスは、デプロイメントリソースとしてデプロイされるべき
  - 冗長性と拡張性のために同一のレプリカを作成
- デプロイメントを公開するには、ロードバランサとなるサービスを利用

## Chapter 2. Developer Workflows

- 開発ワークフローがKubernetesをターゲットにできるようにすることも重要で、これは通常、開発用のクラスタまたはクラスタの少なくとも一部を持つことを意味する。
- Kubernetes向けのアプリケーションを容易に開発できるようにこのようなクラスタをセットアップすることは、Kubernetesで成功を収めるために重要。
- 究極の目標は開発者がKubernetes上で迅速かつ容易にアプリケーションを構築できるようにすること→じゃあどうすれば良い？
  - 第一のフェースはオンボーディング（新しい開発者がチームに参加するとき）
    - ユーザーにクラスタへのログインを許可することと、最初のデプロイメントに向かわせることが含まれる
    - このフェーズの目標は、最小限の時間で開発者の足元を固めること
    - KPIを設けるといい
      - 例えば、何もない状態から HEAD で現在のアプリケーションを 30 分以内に実行できるようにすること
      - チームに新しい人が入るたびに、この目標に対してどうなっているかをテストする
  - 第二のフェーズは開発
    - このフェーズの目標は、迅速な反復作業とデバッグを確実に行うこと
    - 開発者は、素早く繰り返しコードをクラスタにプッシュする必要がある。
    - 開発したコードを簡単にテストし、正しく動作していない場合はデバッグできるようにする必要がある。
    - KPIの測定方法
      - プルリクエスト（PR）や変更をクラスターで稼働させるまでの時間を測定
      - ユーザーが認識している生産性を調査
  - 第三のフェーズはテスト
    - PRを提出する前に、開発者が自分の環境に対するすべてのテストを実行できるようにする
    - コードがリポジトリにマージされる前に、すべてのテストが自動的に実行されるようにすること
    - KPIの測定方法
      - テストの実行にかかる時間
      - テストのフレーキー（実行結果の不安定）さ
        - 開発環境の干渉 (リソースの枯渇など) によっても発生することがある
- Kubernetesでの開発を考え始めると、最初に起こる選択肢の1つが、1つの大きな開発用クラスターを構築するか、開発者1人につき1つのクラスターを持つか
  - ユーザーごとに開発用クラスターを持つことを選択した場合
    - 欠点
      - コストが高くなる
      - 効率が悪くなる
        - 監視・ロギングなどの設定を個々に入れなければならない
      - 多数の異なる開発用クラスターを管理しなければならなくなる
        - 使用されなくなったリソースを追跡して回収することが難しい
    - 利点
      - シンプル
      - 各開発者は自分のクラスタをセルフサービスで管理することができ、孤立している
  - 一つの開発用クラスター
    - 利点、欠点は「ユーザーごと」の逆
    - トータルで見ると一つの開発用クラスターの方が利点が多い
    - 開発クラスターを分けるとしても10-20人単位とかが良さそう
- 大規模なクラスタをセットアップする場合、第一の目標は、複数のユーザーが互いに踏みつけになることなくクラスタを同時に使用できるようにすること
  - namespaceを利用する
  - RBACのスコープにもなり、ある開発者が他の開発者の作業を誤って削除することがないようにすることができる
- ユーザーをKubernetesを利用できるようにする
  - 証明書ベースの認証を使用してユーザー用の新しい証明書を作成し、ログインに使用するkubeconfigファイルを渡す
  - クラスターへのアクセスに外部IDシステム（たとえばIAM）を使用するよう設定する
- 特定のネームスペースが消費するリソースの量を制限したい場合は、ResourceQuota リソースを使用して、特定のネームスペースが消費するリソースの合計数に制限を設定できる
  - コンテナごとの設定は Limit Range
- ネームスペースに開発者を割り当てる方法
  - オンボーディングプロセスの一環として、各ユーザーに自分のネームスペースを割り当てる
    - ユーザーがオンボーディングされた後、常に専用のワークスペースを持ち、そこでアプリケーションを開発および管理できるようになるため、便利
    - 開発者のネームスペースをあまりに永続的にすると、開発者が使い終わった後にネームスペースに物を置いたままにすることを助長し、個々のリソースのゴミ収集や会計処理がより複雑になる
  - TTL（Time To Live）を設定したネームスペースを一時的に作成し、割り当てるという方法
    - 開発者はクラスタ内のリソースを一時的なものとして考えることができ、TTLが切れたときにネームスペース全体を削除する自動化を容易に構築することができる
- 開発クラスタでログ集約をするのは良い考え
- 最初のアプリケーションを立ち上げて実行するためには、自動化よりも慣習が必要
- アプリケーションをデプロイする際の主な課題の1つは、すべての依存関係をインストールすること（マイクロサービスだとデプロイ順序など依存関係たくさんあるよね）
  - すべてのプロジェクトリポジトリーのルートディレクトリ内にsetup.shスクリプトを作成する(Makefileに似たような活動)

      ```sh
      # setup.sh
      kubectl create my-service/database-stateful-set-yaml
      kubectl create my-service/middle-tier.yaml
      kubectl create my-service/configs.yaml
      ```

- 開発者がアプリケーションをすばやく反復できるようにする
  - 最初はコンテナ・イメージをビルドしてプッシュする機能を用意する
  - クラスタにロールアウトする
    - 開発者のイテレーションの場合、可用性を維持することはあまり重要ではない
    - 以前の Deployment に関連付けられた Deployment オブジェクトを削除し、新しくビルドされたイメージを指す新しい Deployment を作成するのが簡単な方法
- Kubernetes上のアプリケーションのデバッグ（って難しいよね。。）
  - kubectl logsからkubectl exec、kubectl port-forward...
  - VSCodeの拡張機能の利用
- 開発者のエクスペリエンスを、オンボーディング、開発、テストの3つのフェーズで考える。構築する開発環境は、この3つのフェーズをすべてサポートしていることを確認する。

## Chapter 3. Monitoring and Logging in Kubernetes

- ブラックボックスモニタリング
  - アプリケーションの外側からのモニタリングに焦点を当て、CPU、メモリ、ストレージなどのコンポーネントのシステムをモニタリング
  - インフラレベルのモニタリングには有効ですが、アプリケーションがどのように動作しているかについての洞察やコンテキストに欠ける
  - クラスタが健全かどうかをテストするためには、ポッドをスケジュールして、それが成功したら、クラスタ内のスケジューラとサービスディスカバリが健全であることがわかるので、クラスタコンポーネントが健全であると仮定することができる（ブラックボックスモニタリングではできないよね）
- ホワイトボックスモニタリング
  - HTTPリクエストの総数、500エラーの数、リクエストのレイテンシなど、アプリケーションの状態のコンテキストにある詳細に焦点を当てる
  - システムの状態の「なぜ」を理解することができる
- Kubernetesでは、Podは非常に動的で短命であるため、この動的で一時的な性質を扱えるような監視を行う必要がある
- 分散システムを監視する際に注目すべき監視パターンはいくつかあります。
  - USE方式
    - USE?
      - U: Utilization
      - S: Saturation
      - E: Errors
    - アプリケーションレベルのモニタリングに使用するには限界があるため、インフラストラクチャのモニタリングに焦点を合わせている
    - システムのリソース制約やエラー率を迅速に把握することができる。
    - クラスタ内のノードのネットワークの健全性をチェックするには、使用率、飽和度、エラー率を監視して、ネットワークのボトルネックやネットワークスタックのエラーを簡単に特定できるようにする必要がある
  - RED方式
    - RED?
      - R: Rate
      - E: Errors
      - D: Duration
    - この考え方は、GoogleのFour Golden Signalsから引用されたもの
      - レイテンシー（リクエストを処理するのにかかる時間）
      - トラフィック（システムにかかる要求の大きさ）
      - エラー（失敗しているリクエストの割合）
      - 飽和（サービスがどれだけ利用されているか）
    - この方法を用いてKubernetesで動いているフロントエンドサービスを監視すると、次のように計算できる。
      - フロントエンドサービスはどれくらいのリクエストを処理しているか？
      - サービスのユーザーが受け取っている500エラーの数はどれくらいか？
      - サービスはリクエストによって過剰に使用されているか？  
    - ユーザーの体験とサービスでの体験に重点を置いている
    - USE メソッドはインフラストラクチャーコンポーネントに焦点を当て、RED メソッドはアプリケーションのエンドユーザーエクスペリエンスのモニタリングに焦点を当てることから、USE と RED メソッドは互いに補完的な関係にあると言える
- Kubernetesクラスタでどのようなコンポーネントを監視すべきか
  - Kubernetesクラスタは、コントロールプレーンコンポーネントとワーカーノードコンポーネントで構成されている。
  - コントロールプレーンコンポーネント
    - API Server
    - etcd
    - scheduler
    - controller manager
  - ワーカーノード
    - kubelet
    - container runtime
    - kube-proxy
    - kube-dns (CoreDNSが使われることも)
    - Pods
  - 健全なクラスタとアプリケーションを確保するために、これらのコンポーネントをすべて監視する必要がある
- クラスター内でメトリクスを収集するために使用できるさまざまなコンポーネント
  - Container Advisor（cAdvisor）
    - kubeletに組み込まれている
    - クラスタの各ノードで実行さる
    - Linux のコントロールグループ（cgroup）ツリーを通じて、メモリと CPU のメトリクスを収集する
    - statfs を通じてディスクメトリクスを収集する
  - Metrics Server
    - kubeletからCPUやメモリなどのリソースメトリクスを収集し格納
    - リソースメトリクスをスケジューラ、HPA（Horizontal Pod Autoscaler）、VPA（Vertical Pod Autoscaler）で利用する
    - Custom Metrics APIにより、監視システムが任意のメトリクスを収集できる
      - たとえばキューサイズなど
      - カスタムなメトリクスを作ってHPAなどにも利用できる
  - kube-state-metrics
    - クラスタに展開されたKubernetesオブジェクトの状況を特定することに重点を置いている
    - いろいろみてるから詳しくは[こちら](https://github.com/kubernetes/kube-state-metrics/tree/master/docs)
    - Pods
      - クラスターにデプロイされているPodの数は？
      - 保留状態のPodはいくつあるか？
      - ポッドのリクエストに対応するのに十分なリソースがあるか？
    - Deployment
      - 実行状態のポッドと希望する状態のポッドの数は？
      - 利用可能なレプリカの数は？
      - どのデプロイメントが更新されたか？
    - Nodes
      - ワーカー・ノードのステータスは？
      - クラスタの割り当て可能なCPUコアは？
      - スケジューラブルでないノードはあるか？
    - Job
      - ジョブはいつ開始されたか？
      - ジョブが完了したのはいつか？
      - 失敗したジョブの数は？
- 何のメトリクスを監視すればよいか？→簡単な答えは「すべて」だが、あまりに多くのことを監視しようとすると、ノイズが多くなり、洞察する必要のある真のシグナルがフィルタリングされてしまう
- Kubernetesのモニタリングについて考えるとき、以下を考慮したレイヤーアプローチを取りたい→モニタリングシステムで正しいシグナルをより簡単に特定することができる→たとえば、Podがペンディング状態になっている場合、ノードのリソース使用率から始めて、問題がなければ、クラスタレベルのコンポーネントをターゲットにする
  - 物理ノードまたは仮想ノード
  - クラスターコンポーネント
  - クラスターアドオン
  - エンドユーザー・アプリケーション
- システムでターゲットとしたいメトリクス
  - Nodes
    - CPU使用率
    - メモリ使用率
    - ネットワーク利用率
    - ディスク使用率
  - クラスタ・コンポーネント
    - etcdのレイテンシー
  - クラスター アドオン
    - Cluster Autoscaler
    - Ingressコントローラ
  - アプリケーション
    - コンテナ・メモリの使用率と飽和状態 コンテナCPUの使用率
    - コンテナ・ネットワークの利用率とエラー・レート
    - アプリケーションフレームワーク固有のメトリクス
- 監視ツールは色々ある
  - Prometheus
  - InfluxDB
  - Datadog
  - Sysdig
  - Cloud provider tools
    - GCP stackdriver
    - Microsoft Azure Monitor for containers
    - AWS Container Insights
- 新しい監視ツールを導入することは、学習曲線とツールの運用実装によるコストが発生するため、常に既に持っている監視ツールを評価すること。
  - 現在、多くの監視ツールがKubernetesに統合されているので、今持っているものが要件を満たすかどうか評価する。
- Kubernetesクラスタとクラスタにデプロイされたアプリケーションからログを収集して一元管理することも必要
- ログについては、「何でもかんでもログを取ればいい」となりがちですが、これでは2つの問題が発生する（入門監視ではまず全部とれと言っていた。（「まず」ね））
  - ノイズが多すぎて、問題を迅速に発見できない
    - デバッグログが必要悪になるため、具体的に何をログに残すべきか、明確な答えがない。
  - ログは多くのリソースを消費し、高いコストがかかる
- 増え続けるログの保存量に対応するために、保存とアーカイブのポリシーを導入する必要がある
  - エンドユーザーの経験では、30日から45日分の履歴ログを持つことが適切です(これは場合による)
- 以下は収集する必要があるログのリスト
  - Node logs
  - Kubernetes control-plane logs
    - API server
    - Controller manager
    - Scheduler
  - Kubernetes audit logs
    - システム内で誰が何をしたかを知ることができるため、セキュリティ監視と考えることができる
    - Kubernetesの監査ログを管理するためのオプションや構成は数多くある。これらの監査ログは非常にノイズが多く、すべてのアクションをログに記録するのはコストがかかる可能性がある。[監査ログのドキュメント](https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/)を見て、自分たちの環境に合わせてこれらのログを微調整できるようにすることを検討する必要がある
  - Application container logs
    - 最も推奨されるやり方はstdoutに吐かせる
    - もう一つのやり方はサイドカーパターン→複数のファイルを監視したい場合
- ログ収集ツール
  - Elastic Stack
  - Datadog
  - Sumo Logic
  - Sysdig
  - Cloud provider services (GCP Stackdriver, Azure Monitor for containers, and Amazon CloudWatch)
- アラートは諸刃の剣であり、何をアラートするか、何を監視すべきかのバランスを取る必要がある
  - アラートを出しすぎると、アラート疲れを起こし、重要なイベントがノイズに埋もれてしまう
  - ポッドに障害が発生したときにアラートを発生させることができる→けどやりたくないよね？
  - Kubernetesの優れた点は、コンテナの健全性を自動的にチェックし、コンテナを自動的に再起動する機能を提供していること
  - 本当に注意したいのは、Service-Level Objectives (SLO) に影響を与えるイベント
    - SLOを設定することで、エンドユーザーとの期待値を設定し、システムがどのように動作すべきかを明確にする。
    - SLOがなければ、ユーザーは自分の意見を形成することができ、それはサービスに対する非現実的な期待になるかもしれない。
- 一般的な監視では、高いCPU使用率、メモリ使用率、またはプロセスが応答しないことに関するアラートに慣れているかもしれない。
  - これらは良いアラートに見えるかもしれませんが、おそらく誰かが直ちに行動を起こす必要があり、オンコールエンジニアに通知する必要がある問題を示しているわけではない。
  - オンコールエンジニアへのアラートは、直ちに人の手が必要で、アプリケーションのUXに影響を及ぼしている問題であるべき
- 即座に対応する必要のないアラートを処理する方法の1つは、原因の改善を自動化することに重点を置くこと
- アラートを構築する際には、アラートの閾値も考慮する必要がある。閾値を短く設定しすぎると、アラートで多くの誤検出が発生する可能性がある
  - 一般的には、誤検出を防ぐために、少なくとも5分間にしきい値を設定することがお勧め
- 標準的な閾値を設定することで、標準を定義し、多くの異なる閾値を細かく管理することを避けることができる。例えば、5分、10分、30分、1時間といった具合に、特定のパターンに従うとよい。
- 大きなグループにアラートを送ると、ユーザーはそれをノイズとみなし、フィルタリングされてしまう傾向がある。
  - 通知は、その問題の責任を負うべきユーザーに送られるべき
  - システムのアラート処理と管理の方法についての詳しい洞察は、Rob Ewaschukによる[「My Philosophy on Alerting」](https://docs.google.com/document/d/199PqyG3UsyXlwieHaqbGiWVa8eMWi8zzAn0YfcApr8Q/edit)を読め
- ベストプラクティスまとめ
  - モニタリング
    - ノードとすべてのKubernetesコンポーネントの利用率、飽和率、エラー率を監視し、アプリケーションのレート、エラー、継続時間を監視する。
    - ブラックボックス監視を使用して、システムの健全性を予測するのではなく、症状を監視する。
    - ホワイトボックス・モニタリングは、インスツルメンテーションを使用してシステムとその内部を検査するために使用する。
    - 時系列ベースのメトリクスを実装し、高精度のメトリクスを得ることで、アプリケーションの挙動を把握することができる。
    - Prometheusのような高次元のキーラベリングを提供するモニタリングシステムを利用する。
    - これにより、影響力のある問題の兆候をより的確に把握することができる。
    - 平均メトリクスを使用して、事実データに基づく小計とメトリクスを視覚化する。
    - 合計メトリクスを使用して、特定のメトリクス間の分布を視覚化する。
  - ロギング
    - 環境の運用状況の全体像を把握するには、メトリクス・モニタリングと組み合わせてロギングを使用する必要がある。
    - 30～45日以上のログの保存には注意が必要です。必要に応じて、より安価なリソースで長期間のアーカイブを行うことができる。
    - サイドカーパターンのログフォワーダーは、より多くのリソースを使用するため、使用を制限する。
    - ログフォワーダーにはDaemonSetを使用し、ログはSTDOUTに送信することを選択する。
  - アラート
    - アラート疲労は、人々やプロセスにおける悪い行動につながる可能性があるため、注意が必要。
    - 常にアラートを段階的に改善することを考え、常に完璧であるとは限らないことを受け入れる。
    - SLO と顧客に影響を与える症状に対して警告を発し、すぐに人間の注意を必要としない一過性の問題には警告を発しない。

## Chapter 4. Configuration, Secrets, and RBAC

- ConfigMapとSecretsによる設定で、コンテナに設定情報を渡すことができる。この2つの主な違いは、Podが受信情報を保存する方法と、データがetcdデータストアに保存される方法
- ConfigMap
  - アプリケーションの要件に非常に適応しやすく、キー/値ペアや、JSON、XML、独自の設定データなどの複雑なバルクデータを提供することができる
  - ConfigMapはPodの設定情報を提供するだけでなく、コントローラ、CRD、オペレータなどのより複雑なシステムサービスで消費される情報を提供することも可能
  - あまり機密性の高くない文字列データのためのもの
- Secrets
  - シークレットデータは簡単に隠せるような方法で保存・処理されるべきで、 環境がそのように設定されている場合は静止時に暗号化される可能性もある
  - 秘密がポッドに注入されると同時に、pod自身は秘密データをプレーンテキストで見ることができる。
  - シークレットデータは少量のデータを意味し、Kubernetesのデフォルトではbase64エンコードされたデータのサイズが1MBに制限されているので、エンコードのオーバーヘッドを考慮して実際のデータは約750KBになるようにする。
  - Kubernetesのシークレットは3種類ある
    - generic
      - これは通常、ファイル、ディレクトリ、または以下のように-from-literal=パラメータを使用して文字列リテラルから作成される通常のキー/値ペアに過ぎない。

        ```sh
        kubectl create secret generic mysecret --from-literal=key1=$3cr3t1 --from-literal=key2=@3cr3t2`
        ```

    - docker registory
      - imagePullsecretがある場合にポッドテンプレートで渡されると、kubeletによって使用され、プライベートDockerレジストリへの認証に必要なクレデンシャルを提供するために使用される

        ```sh
        kubectl create secret docker-registry registryKey --docker-server myreg.azurecr.io --docker-username myreg --docker-password $up3r$3cr3tP@ssw0rd --docker-email ignore@dummy.com
        ```

    - tls
      - これは有効な公開鍵/秘密鍵ペアからTLS（Transport Layer Security）シークレットを作成する。証明書が有効なPEM形式である限り、キーペアは秘密としてエンコードされ、SSL/TLSの必要性に応じて使用するためにポッドに渡すことができる。

        ```sh
        kubectl create secret tls www-tls --key=./path_to_key/wwwtls.key --cert=./path_to_crt/wwwtls.crt
        ```

  - シークレットはシークレットを必要とするポッドを持つノードでのみtmpfsにマウントされ、それを必要とするポッドが無くなると削除される
  - 一見安全そうに見えるが、デフォルトではKubernetesのetcdデータストアに秘密が平文で保存されていることを知っておく必要があり、etcdノード間のmTLSやetcdデータの静止時の暗号化を有効にするなど、システム管理者やクラウドサービスプロバイダがetcd環境のセキュリティを確保するための取り組みをすることが重要。
  - etcdに保持される秘密データを適切に暗号化するために、プロバイダと適切なキーメディアを指定してAPIサーバの構成で構成する必要がある
- ConfigMapやsecretの使用で発生する問題の大半は、オブジェクトが保持するデータが更新されたときに変更がどのように処理されるかについての誤った仮定である。ルールを理解し、そのルールを守りやすくするためのいくつかのトリックを加えることで、トラブルから逃れることができる。
  - 新しいバージョンのポッドを再デプロイすることなくアプリケーションの動的な変更をサポートするには、ConfigMaps/Secrets をボリュームとしてマウントし、ファイルウォッチャーでアプリケーションを構成して、変更されたファイルデータを検出し、必要に応じてreconfigureする。
    - VolumeMounts を使用する場合、考慮すべき点がいくつかある。
      - まず、ConfigMap/Secretが作成されたらすぐに、それをPodの仕様にボリュームとして追加する。そして、そのボリュームをコンテナのファイルシステムにマウントする。ConfigMap/Secretの各プロパティ名がマウントされたディレクトリの新規ファイルとなり、各ファイルの内容がConfigMap/Secretで指定された値となる。
      - 次に、VolumeMounts.subPathプロパティを使用してConfigMap/Secretをマウントすることは避ける。これにより、ConfigMap/Secretを新しいデータで更新した場合に、ボリューム内のデータが動的に更新されるのを防ぐことができる。
  - ConfigMap/Secret は、Pod がデプロイされる前に、それらを消費する Pod のネームスペースに存在する必要がある。オプションのフラグを使用すると、ConfigMap/Secretが存在しない場合にPodが起動しないようにすることができる。
  - 特定の設定データを確保するため、または特定の設定値が設定されていないデプロイを防ぐために、アドミッション・コントローラーを使用する。たとえば、本番環境のJavaワークロードすべてに、特定のJVMプロパティが設定されていることを要求する場合など。PodPresetsというalpha APIがあり、カスタムのアドミッション・コントローラを書かなくても、アノテーションに基づいてConfigMapとsecretをすべてのPodに適用できるようになる。
  - Helmを使ってアプリケーションを環境にリリースしている場合、ライフサイクルフックを使って、ConfigMap/SecretのテンプレートがDeploymentの適用前にデプロイされるようにすることができる。
  - アプリケーションによっては、JSON や YAML ファイルなどの単一のファイルとして設定を適用する必要がある。ConfigMap/Secrets では、`|` 記号を使用することで、生データのブロック全体が使用できる。
  - アプリケーションがシステム環境変数を利用して設定を決める場合、ConfigMap データのインジェクションを利用して、ポッドへの環境変数のマッピングを作ることができます。これを行うには主に2つの方法があります。ConfigMapのすべてのキー/値のペアを一連の環境変数としてenvFromを使用してポッドにマウントし、configMapRefまたはsecretRefを使用するか、configMapKeyRefまたはsecretKeyRefを使用してそれぞれの値で個々のキーを割り当てることです。
  - configMapKeyRefやsecretKeyRefを使う場合は、実際のキーが存在しない場合、ポッドの起動ができなくなるので注意が必要。
  - envFromを使用してConfigMap/Secretからすべてのキーと値のペアをPodに読み込む場合、無効な環境値とみなされるキーはスキップされるが、Podは起動することができる。Podのイベントには、InvalidVariableNamesという理由と、どのキーがスキップされたかという適切なメッセージが含まれる。
  - コンテナにコマンドライン引数を渡す必要がある場合、`$(ENV_KEY)` 補間構文を使用して環境変数データを取得することができる。
  - ConfigMap/Secretデータを環境変数として消費する場合、ConfigMap/Secret内のデータの更新はPod内で更新されず、Podを削除してReplicaSetコントローラに新しいポッドを作成させるか、Deploymentの更新をトリガーして、デプロイメントの仕様で宣言されている適切なアプリケーション更新戦略に従って、Podを再起動することが必要となることを理解することは非常に重要。
  - ConfigMap/Secretへのすべての変更は、デプロイメント全体への更新を必要とすると仮定する方が簡単。これにより、環境変数またはボリュームを使用している場合でも、コードが新しい設定データを取り込むことが保証される。これを簡単にするために、CI/CDパイプラインを使用してConfigMap/Secretのnameプロパティを更新し、デプロイの参照も更新すると、デプロイの通常のKubernetes更新戦略を通じて更新をトリガーすることができる。
- コンピュータシステムのリソースへのアクセスを制限する方法については、数多くの戦略があるが、大半はすべて同じ段階を経ている。飛行機で外国に行くような一般的な経験のアナロジーを使うことで、Kubernetesのようなシステムで起こるプロセスを説明することができる。パスポート、旅行ビザ、税関や国境警備員といった一般的な旅行者の経験を用いて、そのプロセスを示すことができる。
  1. パスポート（被験者認証）。通常は、どこかの政府機関が発行したパスポートが必要で、そのパスポートは、あなたが誰であるかについて、ある種の検証を提供する。これは、Kubernetesにおけるユーザーアカウントに相当する。Kubernetesはユーザー認証のために外部機関に依存しているが、サービスアカウントはKubernetesが直接管理するタイプのアカウントです。
  1. ビザやトラベルポリシー(認可)。各国は、ビザなどの正式な短期協定により、他国のパスポートを保有する旅行者を受け入れることになる。このビザは、特定のビザの種類に応じて、訪問者が何をし、どれくらいの期間訪問国に滞在することができるのかについても概説することになる。これは、Kubernetesにおけるauthorizationに相当する。Kubernetesにはさまざまな認可方法があるが、最も利用されているのはRBACである。これにより、異なるAPI機能に対して非常にきめ細かいアクセスが可能になる。
  1. 国境警備隊や税関（入国管理）。外国に入国する際、通常はパスポートやビザなどの必要書類をチェックし、多くの場合、その国の法律を守っているかどうか、持ち込まれるものを検査する当局機関がある。Kubernetesでは、これがアドミッションコントローラーに相当する。アドミッションコントローラは、定義されたルールやポリシーに基づいて、APIへのリクエストを許可したり、拒否したり、変更したりすることができる。Kubernetesには、PodSecurity、ResourceQuota、ServiceAccountコントローラなど、多くの組み込みアドミッションコントローラがある。Kubernetesはまた、検証またはミューティングアドミッションコントローラを使用することによって、動的なコントローラを可能にする。
- RBACに入門する
  - KubernetesのRBACプロセスには、定義する必要がある3つの主要なコンポーネント、すなわちサブジェクト、ルール、ロールバインディングがある。
    - サブジェクト
      - 実際にアクセスをチェックされる項目。
      - 通常ユーザー、サービスアカウント、またはグループである。
      - 使用する認可モジュールによって処理され、基本認証、x.509クライアント証明書、ベアラートークンなどに分類することができる。最も一般的な実装では、Azure Active Directory（Azure AD）、Salesforce、GoogleなどのOpenID Connectシステムのようなものを使用して、x.509クライアント証明書または何らかのベアラートークンを使用している。
    - ルール
      - 簡単に言うと、API内の特定のオブジェクト（リソース）またはオブジェクトのグループに対して実行可能なアクションの実際のリスト。
      - 典型的なCRUD（Create, Read, Update, Delete）タイプのオペレーションと一致しますが、Kubernetesではwatch, list, execなどの機能が追加されている。
      - オブジェクトは、異なるAPIコンポーネントに合わせ、カテゴリでグループ化されている。
        - 例えばPodオブジェクトはコアAPIの一部であり、apiGroupで参照することができる。
        - 一方、デプロイメントはapp API Groupの下にある。
        - これはRBACプロセスの真の力であり、おそらく適切なRBACコントロールを作成する際に人々を脅かし、混乱させるものである。
    - ロール（ロールは↑に出てきてなかったけど。。）
      - ロールは、定義されたルールのスコープを定義することができる。
      - KubernetesにはroleとclusterRoleという2種類のロールがある
        - role: ネームスペースに固有のロール
        - clusterRole: すべてのネームスペースにまたがるクラスタ全体のロール
    - ロールバインディング
      - RoleBindingは、ユーザやグループなどのサブジェクトを特定のロールにマッピングすることを可能にする。
      - バインディングには、ネームスペースに固有のroleBindingと、クラスタ全体に渡るclusterRoleBindingの2つのモードがある
- RBACのベストプラクティス
  - Kubernetesで実行するために開発されたアプリケーションで、RBACロールとそれに関連するロールバインディングが必要になることはほとんどない。アプリケーションコードが実際にKubernetes APIと直接対話する場合のみ、アプリケーションはRBACの設定を必要とする。
  - もしアプリケーションがKubernetes APIに直接アクセスして、サービスに追加されるエンドポイントに応じて設定を変更したり、特定のnamespaceのすべてのポッドをリストアップする必要がある場合は、Pod specificationで指定される新しいサービスアカウントを作成するのがベストプラクティス。次に、目的を達成するために必要な最小限の特権を持つロールを作成する。
  - ID管理と、必要に応じて二要素認証が可能なOpenID Connectサービスを使用する。これにより、より高度なID認証が可能になる。ユーザーグループを、仕事を達成するために必要な最小限の権限を持つロールにマッピングする。
  - 前述の実践に加え、JIT（Just in Time）アクセスシステムを使用して、SRE、オペレータ、および非常に特定のタスクを達成するために短期間だけ昇格した特権を必要とする可能性がある人々を許可する必要がある。あるいは、これらのユーザは、サインオンをより厳しく監査される別のIDを持つべきであり、これらのアカウントは、役割に結びついたユーザアカウントまたはグループによってより昇格した特権を割り当てられるべきである。
  - KubernetesクラスタにデプロイするCI/CDツールには、特定のサービス・アカウントを使用する必要があるこれにより、クラスタ内の監査可能性を確保し、クラスタ内のオブジェクトを誰がデプロイまたは削除したかを理解することができる。
  - Helmを使用してアプリケーションをデプロイしている場合、デフォルトのサービスアカウントはTillerで、kube-systemにデプロイさる。Tillerを各namespaceにデプロイする場合は、そのnamespaceにスコープされたTiller専用のサービスアカウントを用意した方がよい。Helmのインストール/アップグレードコマンドを呼び出すCI/CDツールでは、プレステップで、サービスアカウントとデプロイのための特定のnamespaceでHelmクライアントを初期化する。サービスアカウント名は各ネームスペースで同じにすることができるが、namespaceは特定のものである必要があります。本書の発行時点で、Helm v3はアルファ版であり、その基本原則の1つは、Tillerはもはや存在しないことを呼びかけておくことが重要。
  - Secrets APIでウォッチとリストを必要とするアプリケーションを制限する。これは基本的に、アプリケーションまたはポッドをデプロイした人がそのnamespace内のシークレットを閲覧できるようにする。もしアプリケーションが特定のシークレットのために Secrets API にアクセスする必要がある場合は、アプリケーションが直接割り当てられたシークレット以外の、読み取る必要のある特定のシークレットの get を使用するよう制限する。


## Chapter 5. Continuous Integration, Testing, and Deployment


## Chapter 6. Versioning, Releases, and Rollouts

## Chapter 7. Worldwide Application Distribution and Staging

## Chapter 8. Resource Management

## Chapter 9. Networking, Network Security, and Service Mesh

## Chapter 10. Pod and Container Security


## Chapter 11. Policy and Governance for Your Cluster

## Chapter 12. Managing Multiple Clusters

## Chapter 13. Integrating External Services and Kubernetes

Burns, Brendan; Villalba, Eddie; Strebel, Dave; Evenson, Lachlan. Kubernetes Best Practices (p.290). O'Reilly Media. Kindle 版. 

## Chapter 14. Running Machine Learning in Kubernetes

Burns, Brendan; Villalba, Eddie; Strebel, Dave; Evenson, Lachlan. Kubernetes Best Practices (p.306). O'Reilly Media. Kindle 版. 

## Chapter 15. Building Higher-Level Application Patterns on Top of Kubernetes

Burns, Brendan; Villalba, Eddie; Strebel, Dave; Evenson, Lachlan. Kubernetes Best Practices (p.325). O'Reilly Media. Kindle 版. 

## Chapter 16. Managing State and Stateful Applications

Burns, Brendan; Villalba, Eddie; Strebel, Dave; Evenson, Lachlan. Kubernetes Best Practices (p.336). O'Reilly Media. Kindle 版. 

## Chapter 17. Admission Control and Authorization

Burns, Brendan; Villalba, Eddie; Strebel, Dave; Evenson, Lachlan. Kubernetes Best Practices (p.354). O'Reilly Media. Kindle 版. 

## Chapter 18. Conclusion

Burns, Brendan; Villalba, Eddie; Strebel, Dave; Evenson, Lachlan. Kubernetes Best Practices (p.371). O'Reilly Media. Kindle 版. 

##