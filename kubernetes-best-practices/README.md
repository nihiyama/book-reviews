# Kubernetes Best Practicesを読んで

## Chapter 1. Setting Up a Basic Service

- セキュリティ対策として公式で用意されているイメージ使え。なるべく小さいイメージ使え（scratchとか）
- コンテナイメージはセマンティックバージョンとSHAハッシュつけろ（例えばv1.0.1-dfha92）
- podのresouceとrequestは合わせておくと予測しやすくて便利
  - ただしリソースは犠牲になる
  - 運用しやすさを取るかリソース消費を取るか
- クラスタの内容とソースが一致しているようにしろ→GitOpsが最善の方法
- 設定情報をアプリケーション自体から分離する→ConfigMap
- ConfigMap自体を変更することでロールアウトしたくなるかもしれないが、これは実際にはベストプラクティスではない
  - より良いアプローチは、ConfigMap自体の名前にバージョン番号を入れることです。変更を加える場合、ConfigMapを更新するのではなく、新しいv2 ConfigMapを作成し、その構成を使用するようにDeploymentリソースを更新する
  - このとき、適切なヘルスチェックと変更間のポーズを使用して、デプロイのロールアウトが自動的にトリガされる。さらに、ロールバックが必要な場合は、v1構成がクラスタに格納されているので、ロールバックはDeploymentを再度更新するだけでよい。
- パスワードはソースコードやコンテイメージに入れるな
- Kubernetesのシークレットは、デフォルトでは暗号化されていない状態で保存される。秘密を暗号化して保存したい場合は、鍵プロバイダと統合して、Kubernetesがクラスタ内のすべての秘密を暗号化するために使用する鍵を提供することができる。これにより、etcdデータベースへの直接攻撃に対する鍵のセキュリティは確保されるが、Kubernetes APIサーバーを介したアクセスが適切に保護されていることを確認する必要があることに注意する。
  - secretの場合、Volumeはtmpfs RAM-backed filesystemとして作成され、その後コンテナにマウントされる。これにより、マシンが物理的に侵害されたとしても（クラウドでは可能性が極めて低いが、データセンターでは可能）、攻撃者が秘密を入手することがはるかに困難になる。
- ステートフルなワークロードを実行する場合、リモートPersistentVolumesを使用して、アプリケーションに関連する状態を管理することが重要
  - ステートフルなサービスはSaaSを使うのが良いとされている
  - ステートのすべての運用要件（バックアップ、データの局所性、冗長性など）や、Kubernetesクラスタにステートがあるとクラスタ間でアプリケーションを移動するのが難しくなるという事実を考慮すると、ほとんどの場合、ストレージSaaSは価格プレミアムに値することは明らか
- 多くのチームにとって最初の失敗モードは、単にファイルをあるクラスタから別のクラスタにコピーしてしまうこと。単一の frontend/ ディレクトリを持つ代わりに、frontend-production/ と frontend-development/ というディレクトリのペアを持つようにする。
- その結果、ほとんどの人はテンプレート・システムを使う->Helm
- ほとんどのサービスは、Deploymentリソースとしてデプロイされるべき
  - 冗長性と拡張性のために同一のレプリカを作成
- Deploymentを公開するには、ロードバランサとなるサービスを利用

## Chapter 2. Developer Workflows

- 開発ワークフローがKubernetesをターゲットにできるようにすることも重要で、これは通常、開発用のクラスタまたはクラスタの少なくとも一部を持つことを意味する。
- Kubernetes向けのアプリケーションを容易に開発できるようにこのようなクラスタをセットアップすることは、Kubernetesで成功を収めるために重要。
- 究極の目標は開発者がKubernetes上で迅速かつ容易にアプリケーションを構築できるようにすること→じゃあどうすれば良い？
  - 第一のフェースはオンボーディング（新しい開発者がチームに参加するとき）
    - ユーザーにクラスタへのログインを許可することと、最初のデプロイメントに向かわせることが含まれる
    - このフェーズの目標は、最小限の時間で開発者の足元を固めること
    - KPIを設けるといい
      - 例えば、何もない状態から HEAD で現在のアプリケーションを 30 分以内に実行できるようにすること
      - チームに新しい人が入るたびに、この目標に対してどうなっているかをテストする
  - 第二のフェーズは開発
    - このフェーズの目標は、迅速な反復作業とデバッグを確実に行うこと
    - 開発者は、素早く繰り返しコードをクラスタにプッシュする必要がある。
    - 開発したコードを簡単にテストし、正しく動作していない場合はデバッグできるようにする必要がある。
    - KPIの測定方法
      - プルリクエスト（PR）や変更をクラスターで稼働させるまでの時間を測定
      - ユーザーが認識している生産性を調査
  - 第三のフェーズはテスト
    - PRを提出する前に、開発者が自分の環境に対するすべてのテストを実行できるようにする
    - コードがリポジトリにマージされる前に、すべてのテストが自動的に実行されるようにすること
    - KPIの測定方法
      - テストの実行にかかる時間
      - テストのフレーキー（実行結果の不安定）さ
        - 開発環境の干渉 (リソースの枯渇など) によっても発生することがある
- Kubernetesでの開発を考え始めると、最初に起こる選択肢の1つが、1つの大きな開発用クラスターを構築するか、開発者1人につき1つのクラスターを持つか
  - ユーザーごとに開発用クラスターを持つことを選択した場合
    - 欠点
      - コストが高くなる
      - 効率が悪くなる
        - 監視・ロギングなどの設定を個々に入れなければならない
      - 多数の異なる開発用クラスターを管理しなければならなくなる
        - 使用されなくなったリソースを追跡して回収することが難しい
    - 利点
      - シンプル
      - 各開発者は自分のクラスタをセルフサービスで管理することができ、孤立している
  - 一つの開発用クラスター
    - 利点、欠点は「ユーザーごと」の逆
    - トータルで見ると一つの開発用クラスターの方が利点が多い
    - 開発クラスターを分けるとしても10-20人単位とかが良さそう
- 大規模なクラスタをセットアップする場合、第一の目標は、複数のユーザーが互いに踏みつけになることなくクラスタを同時に使用できるようにすること
  - namespaceを利用する
  - RBACのスコープにもなり、ある開発者が他の開発者の作業を誤って削除することがないようにすることができる
- ユーザーをKubernetesを利用できるようにする
  - 証明書ベースの認証を使用してユーザー用の新しい証明書を作成し、ログインに使用するkubeconfigファイルを渡す
  - クラスターへのアクセスに外部IDシステム（たとえばIAM）を使用するよう設定する
- 特定のネームスペースが消費するリソースの量を制限したい場合は、ResourceQuota リソースを使用して、特定のネームスペースが消費するリソースの合計数に制限を設定できる
  - コンテナごとの設定は Limit Range
- ネームスペースに開発者を割り当てる方法
  - オンボーディングプロセスの一環として、各ユーザーに自分のネームスペースを割り当てる
    - ユーザーがオンボーディングされた後、常に専用のワークスペースを持ち、そこでアプリケーションを開発および管理できるようになるため、便利
    - 開発者のネームスペースをあまりに永続的にすると、開発者が使い終わった後にネームスペースに物を置いたままにすることを助長し、個々のリソースのゴミ収集や会計処理がより複雑になる
  - TTL（Time To Live）を設定したネームスペースを一時的に作成し、割り当てるという方法
    - 開発者はクラスタ内のリソースを一時的なものとして考えることができ、TTLが切れたときにネームスペース全体を削除する自動化を容易に構築することができる
- 開発クラスタでログ集約をするのは良い考え
- 最初のアプリケーションを立ち上げて実行するためには、自動化よりも慣習が必要
- アプリケーションをデプロイする際の主な課題の1つは、すべての依存関係をインストールすること（マイクロサービスだとデプロイ順序など依存関係たくさんあるよね）
  - すべてのプロジェクトリポジトリーのルートディレクトリ内にsetup.shスクリプトを作成する(Makefileに似たような活動)

      ```sh
      # setup.sh
      kubectl create my-service/database-stateful-set-yaml
      kubectl create my-service/middle-tier.yaml
      kubectl create my-service/configs.yaml
      ```

- 開発者がアプリケーションをすばやく反復できるようにする
  - 最初はコンテナ・イメージをビルドしてプッシュする機能を用意する
  - クラスタにロールアウトする
    - 開発者のイテレーションの場合、可用性を維持することはあまり重要ではない
    - 以前の Deployment に関連付けられた Deployment オブジェクトを削除し、新しくビルドされたイメージを指す新しい Deployment を作成するのが簡単な方法
- Kubernetes上のアプリケーションのデバッグ（って難しいよね。。）
  - kubectl logsからkubectl exec、kubectl port-forward...
  - VSCodeの拡張機能の利用
- 開発者のエクスペリエンスを、オンボーディング、開発、テストの3つのフェーズで考える。構築する開発環境は、この3つのフェーズをすべてサポートしていることを確認する。

## Chapter 3. Monitoring and Logging in Kubernetes

- ブラックボックスモニタリング
  - アプリケーションの外側からのモニタリングに焦点を当て、CPU、メモリ、ストレージなどのコンポーネントのシステムをモニタリング
  - インフラレベルのモニタリングには有効ですが、アプリケーションがどのように動作しているかについての洞察やコンテキストに欠ける
  - クラスタが健全かどうかをテストするためには、ポッドをスケジュールして、それが成功したら、クラスタ内のスケジューラとサービスディスカバリが健全であることがわかるので、クラスタコンポーネントが健全であると仮定することができる（ブラックボックスモニタリングではできないよね）
- ホワイトボックスモニタリング
  - HTTPリクエストの総数、500エラーの数、リクエストのレイテンシなど、アプリケーションの状態のコンテキストにある詳細に焦点を当てる
  - システムの状態の「なぜ」を理解することができる
- Kubernetesでは、Podは非常に動的で短命であるため、この動的で一時的な性質を扱えるような監視を行う必要がある
- 分散システムを監視する際に注目すべき監視パターンはいくつかあります。
  - USE方式
    - USE?
      - U: Utilization
      - S: Saturation
      - E: Errors
    - アプリケーションレベルのモニタリングに使用するには限界があるため、インフラストラクチャのモニタリングに焦点を合わせている
    - システムのリソース制約やエラー率を迅速に把握することができる。
    - クラスタ内のノードのネットワークの健全性をチェックするには、使用率、飽和度、エラー率を監視して、ネットワークのボトルネックやネットワークスタックのエラーを簡単に特定できるようにする必要がある
  - RED方式
    - RED?
      - R: Rate
      - E: Errors
      - D: Duration
    - この考え方は、GoogleのFour Golden Signalsから引用されたもの
      - レイテンシー（リクエストを処理するのにかかる時間）
      - トラフィック（システムにかかる要求の大きさ）
      - エラー（失敗しているリクエストの割合）
      - 飽和（サービスがどれだけ利用されているか）
    - この方法を用いてKubernetesで動いているフロントエンドサービスを監視すると、次のように計算できる。
      - フロントエンドサービスはどれくらいのリクエストを処理しているか？
      - サービスのユーザーが受け取っている500エラーの数はどれくらいか？
      - サービスはリクエストによって過剰に使用されているか？  
    - ユーザーの体験とサービスでの体験に重点を置いている
    - USE メソッドはインフラストラクチャーコンポーネントに焦点を当て、RED メソッドはアプリケーションのエンドユーザーエクスペリエンスのモニタリングに焦点を当てることから、USE と RED メソッドは互いに補完的な関係にあると言える
- Kubernetesクラスタでどのようなコンポーネントを監視すべきか
  - Kubernetesクラスタは、コントロールプレーンコンポーネントとワーカーノードコンポーネントで構成されている。
  - コントロールプレーンコンポーネント
    - API Server
    - etcd
    - scheduler
    - controller manager
  - ワーカーノード
    - kubelet
    - container runtime
    - kube-proxy
    - kube-dns (CoreDNSが使われることも)
    - Pods
  - 健全なクラスタとアプリケーションを確保するために、これらのコンポーネントをすべて監視する必要がある
- クラスター内でメトリクスを収集するために使用できるさまざまなコンポーネント
  - Container Advisor（cAdvisor）
    - kubeletに組み込まれている
    - クラスタの各ノードで実行さる
    - Linux のコントロールグループ（cgroup）ツリーを通じて、メモリと CPU のメトリクスを収集する
    - statfs を通じてディスクメトリクスを収集する
  - Metrics Server
    - kubeletからCPUやメモリなどのリソースメトリクスを収集し格納
    - リソースメトリクスをスケジューラ、HPA（Horizontal Pod Autoscaler）、VPA（Vertical Pod Autoscaler）で利用する
    - Custom Metrics APIにより、監視システムが任意のメトリクスを収集できる
      - たとえばキューサイズなど
      - カスタムなメトリクスを作ってHPAなどにも利用できる
  - kube-state-metrics
    - クラスタに展開されたKubernetesオブジェクトの状況を特定することに重点を置いている
    - いろいろみてるから詳しくは[こちら](https://github.com/kubernetes/kube-state-metrics/tree/master/docs)
    - Pods
      - クラスターにデプロイされているPodの数は？
      - 保留状態のPodはいくつあるか？
      - ポッドのリクエストに対応するのに十分なリソースがあるか？
    - Deployment
      - 実行状態のポッドと希望する状態のポッドの数は？
      - 利用可能なレプリカの数は？
      - どのデプロイメントが更新されたか？
    - Nodes
      - ワーカー・ノードのステータスは？
      - クラスタの割り当て可能なCPUコアは？
      - スケジューラブルでないノードはあるか？
    - Job
      - ジョブはいつ開始されたか？
      - ジョブが完了したのはいつか？
      - 失敗したジョブの数は？
- 何のメトリクスを監視すればよいか？→簡単な答えは「すべて」だが、あまりに多くのことを監視しようとすると、ノイズが多くなり、洞察する必要のある真のシグナルがフィルタリングされてしまう
- Kubernetesのモニタリングについて考えるとき、以下を考慮したレイヤーアプローチを取りたい→モニタリングシステムで正しいシグナルをより簡単に特定することができる→たとえば、Podがペンディング状態になっている場合、ノードのリソース使用率から始めて、問題がなければ、クラスタレベルのコンポーネントをターゲットにする
  - 物理ノードまたは仮想ノード
  - クラスターコンポーネント
  - クラスターアドオン
  - エンドユーザー・アプリケーション
- システムでターゲットとしたいメトリクス
  - Nodes
    - CPU使用率
    - メモリ使用率
    - ネットワーク利用率
    - ディスク使用率
  - クラスタ・コンポーネント
    - etcdのレイテンシー
  - クラスター アドオン
    - Cluster Autoscaler
    - Ingressコントローラ
  - アプリケーション
    - コンテナ・メモリの使用率と飽和状態 コンテナCPUの使用率
    - コンテナ・ネットワークの利用率とエラー・レート
    - アプリケーションフレームワーク固有のメトリクス
- 監視ツールは色々ある
  - Prometheus
  - InfluxDB
  - Datadog
  - Sysdig
  - Cloud provider tools
    - GCP stackdriver
    - Microsoft Azure Monitor for containers
    - AWS Container Insights
- 新しい監視ツールを導入することは、学習曲線とツールの運用実装によるコストが発生するため、常に既に持っている監視ツールを評価すること。
  - 現在、多くの監視ツールがKubernetesに統合されているので、今持っているものが要件を満たすかどうか評価する。
- Kubernetesクラスタとクラスタにデプロイされたアプリケーションからログを収集して一元管理することも必要
- ログについては、「何でもかんでもログを取ればいい」となりがちですが、これでは2つの問題が発生する（入門監視ではまず全部とれと言っていた。（「まず」ね））
  - ノイズが多すぎて、問題を迅速に発見できない
    - デバッグログが必要悪になるため、具体的に何をログに残すべきか、明確な答えがない。
  - ログは多くのリソースを消費し、高いコストがかかる
- 増え続けるログの保存量に対応するために、保存とアーカイブのポリシーを導入する必要がある
  - エンドユーザーの経験では、30日から45日分の履歴ログを持つことが適切です(これは場合による)
- 以下は収集する必要があるログのリスト
  - Node logs
  - Kubernetes control-plane logs
    - API server
    - Controller manager
    - Scheduler
  - Kubernetes audit logs
    - システム内で誰が何をしたかを知ることができるため、セキュリティ監視と考えることができる
    - Kubernetesの監査ログを管理するためのオプションや構成は数多くある。これらの監査ログは非常にノイズが多く、すべてのアクションをログに記録するのはコストがかかる可能性がある。[監査ログのドキュメント](https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/)を見て、自分たちの環境に合わせてこれらのログを微調整できるようにすることを検討する必要がある
  - Application container logs
    - 最も推奨されるやり方はstdoutに吐かせる
    - もう一つのやり方はサイドカーパターン→複数のファイルを監視したい場合
- ログ収集ツール
  - Elastic Stack
  - Datadog
  - Sumo Logic
  - Sysdig
  - Cloud provider services (GCP Stackdriver, Azure Monitor for containers, and Amazon CloudWatch)
- アラートは諸刃の剣であり、何をアラートするか、何を監視すべきかのバランスを取る必要がある
  - アラートを出しすぎると、アラート疲れを起こし、重要なイベントがノイズに埋もれてしまう
  - ポッドに障害が発生したときにアラートを発生させることができる→けどやりたくないよね？
  - Kubernetesの優れた点は、コンテナの健全性を自動的にチェックし、コンテナを自動的に再起動する機能を提供していること
  - 本当に注意したいのは、Service-Level Objectives (SLO) に影響を与えるイベント
    - SLOを設定することで、エンドユーザーとの期待値を設定し、システムがどのように動作すべきかを明確にする。
    - SLOがなければ、ユーザーは自分の意見を形成することができ、それはサービスに対する非現実的な期待になるかもしれない。
- 一般的な監視では、高いCPU使用率、メモリ使用率、またはプロセスが応答しないことに関するアラートに慣れているかもしれない。
  - これらは良いアラートに見えるかもしれませんが、おそらく誰かが直ちに行動を起こす必要があり、オンコールエンジニアに通知する必要がある問題を示しているわけではない。
  - オンコールエンジニアへのアラートは、直ちに人の手が必要で、アプリケーションのUXに影響を及ぼしている問題であるべき
- 即座に対応する必要のないアラートを処理する方法の1つは、原因の改善を自動化することに重点を置くこと
- アラートを構築する際には、アラートの閾値も考慮する必要がある。閾値を短く設定しすぎると、アラートで多くの誤検出が発生する可能性がある
  - 一般的には、誤検出を防ぐために、少なくとも5分間にしきい値を設定することがお勧め
- 標準的な閾値を設定することで、標準を定義し、多くの異なる閾値を細かく管理することを避けることができる。例えば、5分、10分、30分、1時間といった具合に、特定のパターンに従うとよい。
- 大きなグループにアラートを送ると、ユーザーはそれをノイズとみなし、フィルタリングされてしまう傾向がある。
  - 通知は、その問題の責任を負うべきユーザーに送られるべき
  - システムのアラート処理と管理の方法についての詳しい洞察は、Rob Ewaschukによる[「My Philosophy on Alerting」](https://docs.google.com/document/d/199PqyG3UsyXlwieHaqbGiWVa8eMWi8zzAn0YfcApr8Q/edit)を読め
- ベストプラクティスまとめ
  - モニタリング
    - ノードとすべてのKubernetesコンポーネントの利用率、飽和率、エラー率を監視し、アプリケーションのレート、エラー、継続時間を監視する。
    - ブラックボックス監視を使用して、システムの健全性を予測するのではなく、症状を監視する。
    - ホワイトボックス・モニタリングは、インスツルメンテーションを使用してシステムとその内部を検査するために使用する。
    - 時系列ベースのメトリクスを実装し、高精度のメトリクスを得ることで、アプリケーションの挙動を把握することができる。
    - Prometheusのような高次元のキーラベリングを提供するモニタリングシステムを利用する。
    - これにより、影響力のある問題の兆候をより的確に把握することができる。
    - 平均メトリクスを使用して、事実データに基づく小計とメトリクスを視覚化する。
    - 合計メトリクスを使用して、特定のメトリクス間の分布を視覚化する。
  - ロギング
    - 環境の運用状況の全体像を把握するには、メトリクス・モニタリングと組み合わせてロギングを使用する必要がある。
    - 30～45日以上のログの保存には注意が必要です。必要に応じて、より安価なリソースで長期間のアーカイブを行うことができる。
    - サイドカーパターンのログフォワーダーは、より多くのリソースを使用するため、使用を制限する。
    - ログフォワーダーにはDaemonSetを使用し、ログはSTDOUTに送信することを選択する。
  - アラート
    - アラート疲労は、人々やプロセスにおける悪い行動につながる可能性があるため、注意が必要。
    - 常にアラートを段階的に改善することを考え、常に完璧であるとは限らないことを受け入れる。
    - SLO と顧客に影響を与える症状に対して警告を発し、すぐに人間の注意を必要としない一過性の問題には警告を発しない。

## Chapter 4. Configuration, Secrets, and RBAC

- ConfigMapとSecretsによる設定で、コンテナに設定情報を渡すことができる。この2つの主な違いは、Podが受信情報を保存する方法と、データがetcdデータストアに保存される方法
- ConfigMap
  - アプリケーションの要件に非常に適応しやすく、キー/値ペアや、JSON、XML、独自の設定データなどの複雑なバルクデータを提供することができる
  - ConfigMapはPodの設定情報を提供するだけでなく、コントローラ、CRD、オペレータなどのより複雑なシステムサービスで消費される情報を提供することも可能
  - あまり機密性の高くない文字列データのためのもの
- Secrets
  - シークレットデータは簡単に隠せるような方法で保存・処理されるべきで、 環境がそのように設定されている場合は静止時に暗号化される可能性もある
  - 秘密がポッドに注入されると同時に、pod自身は秘密データをプレーンテキストで見ることができる。
  - シークレットデータは少量のデータを意味し、Kubernetesのデフォルトではbase64エンコードされたデータのサイズが1MBに制限されているので、エンコードのオーバーヘッドを考慮して実際のデータは約750KBになるようにする。
  - Kubernetesのシークレットは3種類ある
    - generic
      - これは通常、ファイル、ディレクトリ、または以下のように-from-literal=パラメータを使用して文字列リテラルから作成される通常のキー/値ペアに過ぎない。

        ```sh
        kubectl create secret generic mysecret --from-literal=key1=$3cr3t1 --from-literal=key2=@3cr3t2`
        ```

    - docker registory
      - imagePullsecretがある場合にポッドテンプレートで渡されると、kubeletによって使用され、プライベートDockerレジストリへの認証に必要なクレデンシャルを提供するために使用される

        ```sh
        kubectl create secret docker-registry registryKey --docker-server myreg.azurecr.io --docker-username myreg --docker-password $up3r$3cr3tP@ssw0rd --docker-email ignore@dummy.com
        ```

    - tls
      - これは有効な公開鍵/秘密鍵ペアからTLS（Transport Layer Security）シークレットを作成する。証明書が有効なPEM形式である限り、キーペアは秘密としてエンコードされ、SSL/TLSの必要性に応じて使用するためにポッドに渡すことができる。

        ```sh
        kubectl create secret tls www-tls --key=./path_to_key/wwwtls.key --cert=./path_to_crt/wwwtls.crt
        ```

  - シークレットはシークレットを必要とするポッドを持つノードでのみtmpfsにマウントされ、それを必要とするポッドが無くなると削除される
  - 一見安全そうに見えるが、デフォルトではKubernetesのetcdデータストアに秘密が平文で保存されていることを知っておく必要があり、etcdノード間のmTLSやetcdデータの静止時の暗号化を有効にするなど、システム管理者やクラウドサービスプロバイダがetcd環境のセキュリティを確保するための取り組みをすることが重要。
  - etcdに保持される秘密データを適切に暗号化するために、プロバイダと適切なキーメディアを指定してAPIサーバの構成で構成する必要がある
- ConfigMapやsecretの使用で発生する問題の大半は、オブジェクトが保持するデータが更新されたときに変更がどのように処理されるかについての誤った仮定である。ルールを理解し、そのルールを守りやすくするためのいくつかのトリックを加えることで、トラブルから逃れることができる。
  - 新しいバージョンのポッドを再デプロイすることなくアプリケーションの動的な変更をサポートするには、ConfigMaps/Secrets をボリュームとしてマウントし、ファイルウォッチャーでアプリケーションを構成して、変更されたファイルデータを検出し、必要に応じてreconfigureする。
    - VolumeMounts を使用する場合、考慮すべき点がいくつかある。
      - まず、ConfigMap/Secretが作成されたらすぐに、それをPodの仕様にボリュームとして追加する。そして、そのボリュームをコンテナのファイルシステムにマウントする。ConfigMap/Secretの各プロパティ名がマウントされたディレクトリの新規ファイルとなり、各ファイルの内容がConfigMap/Secretで指定された値となる。
      - 次に、VolumeMounts.subPathプロパティを使用してConfigMap/Secretをマウントすることは避ける。これにより、ConfigMap/Secretを新しいデータで更新した場合に、ボリューム内のデータが動的に更新されるのを防ぐことができる。
  - ConfigMap/Secret は、Pod がデプロイされる前に、それらを消費する Pod のネームスペースに存在する必要がある。オプションのフラグを使用すると、ConfigMap/Secretが存在しない場合にPodが起動しないようにすることができる。
  - 特定の設定データを確保するため、または特定の設定値が設定されていないデプロイを防ぐために、アドミッション・コントローラーを使用する。たとえば、本番環境のJavaワークロードすべてに、特定のJVMプロパティが設定されていることを要求する場合など。PodPresetsというalpha APIがあり、カスタムのアドミッション・コントローラを書かなくても、アノテーションに基づいてConfigMapとsecretをすべてのPodに適用できるようになる。
  - Helmを使ってアプリケーションを環境にリリースしている場合、ライフサイクルフックを使って、ConfigMap/SecretのテンプレートがDeploymentの適用前にデプロイされるようにすることができる。
  - アプリケーションによっては、JSON や YAML ファイルなどの単一のファイルとして設定を適用する必要がある。ConfigMap/Secrets では、`|` 記号を使用することで、生データのブロック全体が使用できる。
  - アプリケーションがシステム環境変数を利用して設定を決める場合、ConfigMap データのインジェクションを利用して、ポッドへの環境変数のマッピングを作ることができます。これを行うには主に2つの方法があります。ConfigMapのすべてのキー/値のペアを一連の環境変数としてenvFromを使用してポッドにマウントし、configMapRefまたはsecretRefを使用するか、configMapKeyRefまたはsecretKeyRefを使用してそれぞれの値で個々のキーを割り当てることです。
  - configMapKeyRefやsecretKeyRefを使う場合は、実際のキーが存在しない場合、ポッドの起動ができなくなるので注意が必要。
  - envFromを使用してConfigMap/Secretからすべてのキーと値のペアをPodに読み込む場合、無効な環境値とみなされるキーはスキップされるが、Podは起動することができる。Podのイベントには、InvalidVariableNamesという理由と、どのキーがスキップされたかという適切なメッセージが含まれる。
  - コンテナにコマンドライン引数を渡す必要がある場合、`$(ENV_KEY)` 補間構文を使用して環境変数データを取得することができる。
  - ConfigMap/Secretデータを環境変数として消費する場合、ConfigMap/Secret内のデータの更新はPod内で更新されず、Podを削除してReplicaSetコントローラに新しいポッドを作成させるか、Deploymentの更新をトリガーして、デプロイメントの仕様で宣言されている適切なアプリケーション更新戦略に従って、Podを再起動することが必要となることを理解することは非常に重要。
  - ConfigMap/Secretへのすべての変更は、デプロイメント全体への更新を必要とすると仮定する方が簡単。これにより、環境変数またはボリュームを使用している場合でも、コードが新しい設定データを取り込むことが保証される。これを簡単にするために、CI/CDパイプラインを使用してConfigMap/Secretのnameプロパティを更新し、デプロイの参照も更新すると、デプロイの通常のKubernetes更新戦略を通じて更新をトリガーすることができる。
- コンピュータシステムのリソースへのアクセスを制限する方法については、数多くの戦略があるが、大半はすべて同じ段階を経ている。飛行機で外国に行くような一般的な経験のアナロジーを使うことで、Kubernetesのようなシステムで起こるプロセスを説明することができる。パスポート、旅行ビザ、税関や国境警備員といった一般的な旅行者の経験を用いて、そのプロセスを示すことができる。
  1. パスポート（被験者認証）。通常は、どこかの政府機関が発行したパスポートが必要で、そのパスポートは、あなたが誰であるかについて、ある種の検証を提供する。これは、Kubernetesにおけるユーザーアカウントに相当する。Kubernetesはユーザー認証のために外部機関に依存しているが、サービスアカウントはKubernetesが直接管理するタイプのアカウントです。
  1. ビザやトラベルポリシー(認可)。各国は、ビザなどの正式な短期協定により、他国のパスポートを保有する旅行者を受け入れることになる。このビザは、特定のビザの種類に応じて、訪問者が何をし、どれくらいの期間訪問国に滞在することができるのかについても概説することになる。これは、Kubernetesにおけるauthorizationに相当する。Kubernetesにはさまざまな認可方法があるが、最も利用されているのはRBACである。これにより、異なるAPI機能に対して非常にきめ細かいアクセスが可能になる。
  1. 国境警備隊や税関（入国管理）。外国に入国する際、通常はパスポートやビザなどの必要書類をチェックし、多くの場合、その国の法律を守っているかどうか、持ち込まれるものを検査する当局機関がある。Kubernetesでは、これがアドミッションコントローラーに相当する。アドミッションコントローラは、定義されたルールやポリシーに基づいて、APIへのリクエストを許可したり、拒否したり、変更したりすることができる。Kubernetesには、PodSecurity、ResourceQuota、ServiceAccountコントローラなど、多くの組み込みアドミッションコントローラがある。Kubernetesはまた、検証またはミューティングアドミッションコントローラを使用することによって、動的なコントローラを可能にする。
- RBACに入門する
  - KubernetesのRBACプロセスには、定義する必要がある3つの主要なコンポーネント、すなわちサブジェクト、ルール、ロールバインディングがある。
    - サブジェクト
      - 実際にアクセスをチェックされる項目。
      - 通常ユーザー、サービスアカウント、またはグループである。
      - 使用する認可モジュールによって処理され、基本認証、x.509クライアント証明書、ベアラートークンなどに分類することができる。最も一般的な実装では、Azure Active Directory（Azure AD）、Salesforce、GoogleなどのOpenID Connectシステムのようなものを使用して、x.509クライアント証明書または何らかのベアラートークンを使用している。
    - ルール
      - 簡単に言うと、API内の特定のオブジェクト（リソース）またはオブジェクトのグループに対して実行可能なアクションの実際のリスト。
      - 典型的なCRUD（Create, Read, Update, Delete）タイプのオペレーションと一致しますが、Kubernetesではwatch, list, execなどの機能が追加されている。
      - オブジェクトは、異なるAPIコンポーネントに合わせ、カテゴリでグループ化されている。
        - 例えばPodオブジェクトはコアAPIの一部であり、apiGroupで参照することができる。
        - 一方、デプロイメントはapp API Groupの下にある。
        - これはRBACプロセスの真の力であり、おそらく適切なRBACコントロールを作成する際に人々を脅かし、混乱させるものである。
    - ロール（ロールは↑に出てきてなかったけど。。）
      - ロールは、定義されたルールのスコープを定義することができる。
      - KubernetesにはroleとclusterRoleという2種類のロールがある
        - role: ネームスペースに固有のロール
        - clusterRole: すべてのネームスペースにまたがるクラスタ全体のロール
    - ロールバインディング
      - RoleBindingは、ユーザやグループなどのサブジェクトを特定のロールにマッピングすることを可能にする。
      - バインディングには、ネームスペースに固有のroleBindingと、クラスタ全体に渡るclusterRoleBindingの2つのモードがある
- RBACのベストプラクティス
  - Kubernetesで実行するために開発されたアプリケーションで、RBACロールとそれに関連するロールバインディングが必要になることはほとんどない。アプリケーションコードが実際にKubernetes APIと直接対話する場合のみ、アプリケーションはRBACの設定を必要とする。
  - もしアプリケーションがKubernetes APIに直接アクセスして、サービスに追加されるエンドポイントに応じて設定を変更したり、特定のnamespaceのすべてのポッドをリストアップする必要がある場合は、Pod specificationで指定される新しいサービスアカウントを作成するのがベストプラクティス。次に、目的を達成するために必要な最小限の特権を持つロールを作成する。
  - ID管理と、必要に応じて二要素認証が可能なOpenID Connectサービスを使用する。これにより、より高度なID認証が可能になる。ユーザーグループを、仕事を達成するために必要な最小限の権限を持つロールにマッピングする。
  - 前述の実践に加え、JIT（Just in Time）アクセスシステムを使用して、SRE、オペレータ、および非常に特定のタスクを達成するために短期間だけ昇格した特権を必要とする可能性がある人々を許可する必要がある。あるいは、これらのユーザは、サインオンをより厳しく監査される別のIDを持つべきであり、これらのアカウントは、役割に結びついたユーザアカウントまたはグループによってより昇格した特権を割り当てられるべきである。
  - KubernetesクラスタにデプロイするCI/CDツールには、特定のサービス・アカウントを使用する必要があるこれにより、クラスタ内の監査可能性を確保し、クラスタ内のオブジェクトを誰がデプロイまたは削除したかを理解することができる。
  - Helmを使用してアプリケーションをデプロイしている場合、デフォルトのサービスアカウントはTillerで、kube-systemにデプロイさる。Tillerを各namespaceにデプロイする場合は、そのnamespaceにスコープされたTiller専用のサービスアカウントを用意した方がよい。Helmのインストール/アップグレードコマンドを呼び出すCI/CDツールでは、プレステップで、サービスアカウントとデプロイのための特定のnamespaceでHelmクライアントを初期化する。サービスアカウント名は各ネームスペースで同じにすることができるが、namespaceは特定のものである必要があります。本書の発行時点で、Helm v3はアルファ版であり、その基本原則の1つは、Tillerはもはや存在しないことを呼びかけておくことが重要。
  - Secrets APIでウォッチとリストを必要とするアプリケーションを制限する。これは基本的に、アプリケーションまたはポッドをデプロイした人がそのnamespace内のシークレットを閲覧できるようにする。もしアプリケーションが特定のシークレットのために Secrets API にアクセスする必要がある場合は、アプリケーションが直接割り当てられたシークレット以外の、読み取る必要のある特定のシークレットの get を使用するよう制限する。

## Chapter 5. Continuous Integration, Testing, and Deployment

- CI/CDの目標は、開発者がコードをチェックインしてから、新しいコードを本番環境にロールアウトするまで、完全に自動化されたプロセスを持つこと
- Kubernetesにデプロイされたアプリの更新を手動でロールアウトすることは、非常にエラーが発生しやすいので避けたい
- 重要なトピック
  - バージョン管理
    - すべてのCI/CDパイプラインは、アプリケーションと構成のコード変更の実行履歴を維持するバージョン管理から始まる
    - ブランチ戦略を立てるにはさまざまな方法があるが、組織構造や職務の分担を考えてね
    - アプリケーションコードとKubernetesマニフェストやHelmチャートなどの設定コードの両方を含めることで、コミュニケーションとコラボレーションという優れたDevOpsの原則を促進することができる
    - アプリケーション開発者と運用エンジニアの両方が単一のリポジトリでコラボレーションすることで、アプリケーションを本番に提供するチームへの信頼が高まる
  - CI (Continuous Integration)
    - 大きな変更をあまり頻繁にコミットしない代わりに、小さな変更をより頻繁にコミットする
    - コード変更がリポジトリにコミットされるたびに、ビルドが開始される。これにより、実際に問題が発生した場合に、何がアプリケーションを壊してしまったのかについて、より迅速なフィードバックループを持つことができるようになる
    - なぜ、アプリケーションがどのようにビルドされているかを知る必要があるのか、それはアプリケーション開発者の役割ではないのか？従来はそうだったかもしれないが、企業がDevOps文化を受け入れる方向に進むにつれて、運用チームはアプリケーションコードやソフトウェア開発のワークフローに近づくようになった
  - テスト
    - パイプラインでテストを実行する目的は、ビルドを破壊するコード変更に対するフィードバックループを迅速に提供すること
    - コードベースに対して失敗するテストがある場合、コンテナイメージをビルドしてレジストリにプッシュすることは避けたい
    - インフラストラクチャやアプリケーションの本番環境への配信を自動化するようになると、コードベースのすべての部分に対して自動テストを実行することを考える必要がある
      - つまりyamlに対してもテストする
  - イメージのタグ付け
    - まず、小さいイメージを作る必要がある（これはセキュリティの観点からも良い）
      - multistage build
        - アプリケーションの実行に必要ない依存関係を削除することができる
        - golangだとbuildしたバイナリだけあれば良くて、ソースコードは必要ない
      - distroless base image
        - イメージから不要なバイナリやシェルをすべて削除したもの
        - シェルがないため、イメージにデバッガをアタッチできないのが難点
      - optimized base image
        - OS 層から不要なものを取り除き、スリム化することに重点を置いたimage
        - slim系のイメージとかかな
        - 割と最適解だったりする
        - とはいえ最終的にdistrolessイメージにできるといいね
    - タグづけ
      - latestタグを使うな
      - 代表的な戦略は以下の通り
        - BuildID
          - CIがキックされる時のID
        - Build System-BuildID
          - 複数のビルドシステムを持っているユーザー向け
        - Git Hash
          - コードがコミットされた時のhash値
          - タグづけビルドもこれにあたるかな？
        - githash-buildID
          - git hashとbuildidの両方を使うやり方
  - CD (Continuous Deployment)
    - CDは、CIパイプラインを正常に通過した変更を、人手を介さずに本番環境にデプロイするプロセス
    - KubernetesはDeploymentオブジェクトを宣言的に記述し、一貫した方法でバージョン管理、デプロイを行うことができる
    - しっかりとしたCIパイプラインをセットアップする必要はあるよ
  - デプロイメント戦略
    - ローリングアップデート
      - ロールアウト中に更新されるレプリカの最大量と利用できないポッドの最大量を設定することができる

        ```yaml
        kind: Deployment
        apiVersion: v1
        metadata:
          name: frontend
        spec:
          replicas: 3
          template:
            spec:
              containers:
                - name: frontend
                  image: brendanburns/frontend:v1
          strategy:
              type: RollingUpdate
              rollingUpdate:
              maxSurge: 1 # Maximum amount of replicas to update at one time
              maxUnavailable: 1 # Maximum amount of replicas unavailable during rollout
        ```

      - この戦略を使うと、接続が切断される可能性があるので注意が必要→readiness probeとpreStop lifecycle hookの利用が効果的

        ```yaml
        kind: Deployment
          apiVersion: v1
          metadata:
          name: frontend
          spec:
            replicas: 3
            template:
              spec:
                containers:
                - name: frontend
                  image: brendanburns/frontend:v1
                livenessProbe:
                  # ...
                readinessProbe:
                  httpGet:
                    path: /readiness # probe endpoint
                    port: 8888
                lifecycle:
                  preStop:
                    exec:
                      command: ["/usr/sbin/nginx","-s","quit"]
          strategy:
            # ...
        ```

        - readiness probeは、デプロイされた新バージョンがトラフィックを受け入れる準備ができているか確認するもの
        - preStopフックは現在デプロイされているアプリケーションでコネクションが枯渇していることを確認することができる
        - ライフサイクルフックはコンテナが終了する前に呼び出され、同期的に実行されるため、最終的な終了シグナルが与えられる前に完了する
        - preStopライフサイクルフックは NGINX をgracefulに終了させるが、SIGTERM はgracefulでない、素早い終了を実施します
      - ローリングアップデートのもう一つの懸念は、ロールオーバー中に2つのバージョンのアプリケーションを同時に実行されること
        - データベーススキーマは、両方のバージョンのアプリケーションをサポートする必要がある
          - バージョンアップではスキーマ変更は常に追加する形がいい（削除は次の次のバージョンくらいで行う）
      - readiness probeとliveness probeを用意する
        - readiness probe: エンドポイントとしてサービスの背後に置く前に、アプリケーションがトラフィックに対応する準備ができていることを確認する
        - liveness probe: アプリケーションが健全に動作していることを確認し、そのlivenessプローブに失敗した場合にポッドを再起動させる
    - Blue/Green デプロイメント
      - 予測可能な方法でアプリケーションをリリースすることができる
      - トラフィックが新しい環境に移行するタイミングを制御できるため、アプリケーションの新バージョンのロールアウトをかなり制御できるようになる
      - 以前のバージョンのアプリケーションに簡単に切り替えることができる
      - 考慮しなければならないことはいくつかある
        - インフライトトランザクション（実行中のトランザクション）とスキーマ更新の互換性を考慮する必要があるため、データベースのマイグレーションが困難になる
        - 両方の環境を誤って削除してしまう危険性がある
        - 両方の環境に対して余分な容量が必要=コスト
        - レガシーアプリがデプロイメントを処理できないハイブリッドデプロイメントでは、調整の問題がある
    - カナリアリリース
      - Blue/Green デプロイメントに似ているが、詳細にトラフィックを制御できる
      - 一部のユーザーのみに新しいのを利用させるというのもできる
      - デプロイの失敗や機能の破損のリスクを、より少数のユーザーに対して低減することができる
      - 問題なければ徐々にトラフィックを徐々に新の方に増やすというやり方ができる(SRE本ではイクスポーネンシャルの関数でやると大体良いって書いてあったな)
      - Blue/Greenに加えてさらに以下の考慮が必要
        - トラフィックをある割合のユーザーにシフトする能力  
        - 新リリースと比較するための定常状態に関する確かな知識  
        - 新しいリリースが「良い」状態なのか「悪い」状態なのかを理解するための指標
      - 複数のバージョンのアプリケーションを同時に実行することに苦しむ
        - データベーススキーマは必ず両方のバージョンを用意する必要がある
  - デプロイメントのテスト  
    - 本番環境でのテストは、アプリケーションの回復力、スケーラビリティ、およびUXに対する信頼性を高めるのに役立つ
    - 実運用環境でのテストには課題やリスクがつきものだが、システムの信頼性を確保するために努力する価値がある
    - 実運用環境でのテストの影響を特定できるような、綿密な観測可能性戦略を確立しておく必要がある
    - エンドユーザーのアプリケーション体験に影響を与える指標を観察できなければ、システムの耐障害性を向上させるために何に焦点を当てるべきか、明確な指標を得ることはできない→つまりSLOちゃんと決めとけ
    - ツールは色々ある
      - Canary deployments  
      - A/B testing  
      - Traffic shifting  
      - Feature flags  
  - カオステスト
    - カオスエンジニアリングは、Netflixによって開発された
    - カオスエンジニアリングとは、本番システムに実験を導入し、システムの弱点を発見する手法
    - 「ゲームデイ」実験をやるといい
      1. 仮説を立て、定常状態について学ぶ。
      1. システムに影響を与える可能性のある現実の事象を様々な程度で用意する。
      1. 制御グループを作り、定常状態と比較する実験をする。
      1. 実験を行い、仮説を立てる。
    - なぜステージングでテストしないのか？→本番固有の問題に気づけない
      - リソースの非同期配置  
      - 本番環境からの設定変更
      - トラフィックとユーザー行動は合成的に生成される傾向がある。
      - 生成されるリクエストの数が実際のワークロードを模倣していない。
      - ステージングに実装されたモニタリングの欠如。
      - 展開されたデータサービスには、本番環境とは異なるデータや負荷が含まれている。
- CI/CDのやり方
  - アプリケーションコードのビルドを実行する  
  - コードに対するテストの実行  
  - テストに成功したら、コンテナイメージを構築する コンテナイメージをコンテナレジストリにプッシュする
  - Kubernetesにアプリケーションをデプロイする
  - デプロイされたアプリケーションに対してテストを実行する
  - デプロイメントに対するローリングアップグレードの実行
- CI/CDのベストプラクティス
  - CIでは、自動化と迅速なビルドを提供することに重点を置く。ビルド速度を最適化することで、開発者の変更がビルドを破壊した場合に、開発者に迅速なフィードバックを提供することができる。
  - パイプラインでは、信頼性の高いテストを提供することに重点を置く。これにより、開発者は自分のコードの問題点を迅速にフィードバックすることができる。開発者へのフィードバックのループが速ければ速いほど、ワークフローの生産性が向上する。
  - CI/CDツールを決定する際には、パイプラインをコードとして定義できるツールであることを確認する。これにより、パイプラインをアプリケーションのコードと一緒にバージョン管理することができる。
  - イメージを最適化することで、イメージのサイズを小さくし、本番環境でイメージを実行する際の攻撃対象領域を小さくすることができることを確認する。multi stage Dockerビルドでは、アプリケーションの実行に必要ないパッケージを削除することができる。例えば、アプリケーションをビルドするためにMavenが必要でも、実際に実行するイメージには必要ない場合がある。
  - イメージのタグとして "latest" を使うのは避け、buildID や Git commit を参照できるタグを利用する。
  - CDに不慣れな方は、まずKubernetesのローリングアップグレードを活用する。これは使いやすく、デプロイに慣れることができる。CDに慣れ、自信がついてきたら、Blue/Greenやカナリアデプロイメント戦略を活用することも視野に入れる。
  - CDでは、クライアント接続とデータベーススキーマのアップグレードがアプリケーションでどのように処理されるかを確実にテストする。
  - 実運用環境でのテストは、アプリケーションに信頼性を持たせ、適切な監視を行うのに役立つ。実稼働環境でのテストでは、小規模から始めて、実験の爆発半径を制限する。

## Chapter 6. Versioning, Releases, and Rollouts

- 従来のモノリシック・アプリケーションの主な不満点の1つは、時間が経つにつれて規模が大きくなりすぎて、ビジネスが求めるスピードで適切にアップグレード、バージョンアップ、または修正を行うことができなくなること
- 新しいコードを素早く反復し、新しい問題を解決し、隠れた問題が大きな問題になる前に修正できること、そしてダウンタイムゼロのアップグレードを約束することは、この変化し続けるインターネット経済の世界で開発チームが目指す目標
- バージョニングについて
  - 重要なのは、パターンを選んで、それにこだわること
  - 基本的にセマンティックバージョニングが最も有用
    - `v<メジャー>.<マイナー>.<パッチ>`
    - メジャー: APIの互換性がない
    - マイナー: APIの互換性がある
    - パッチ: バグを直しただけ
  - 最も重要なことは、システム全体に一貫性があること
- リリースについて
  - Kubernetesにはリリースコントローラが存在しないため、リリースというネイティブな概念はない
    - Deploymentのmetadata.labels specおよび/またはpod.spec.template.metadata.label specで追加される
  - 重要なのは、バージョン管理の方法と、クラスタのシステム状態のどこに表示されるかの一貫性
  - リリース名は、特定の名前の定義について組織的な合意があれば、非常に有用
    - 多くの場合、stable や canary といったラベルが使用され、サービスメッシュのようなツールを追加して細かいルーティングを決定する際に、ある種の運用管理を行うのに役立つ
  - ラベル自体は非常に自由な形式で、APIの構文規則に従った任意のキーと値のペアにすることができる。重要なのは内容ではなく、各コントローラがどのようにラベルを処理するか、ラベルの変更、ラベルのセレクタマッチングを行うか
    - Deployments, ReplicaSets, DaemonSets は、直接のマッピングやセットベースの式によるラベルを介したポッドのセレクタベースのマッチングをサポートする
    - つまり、新しいセレクタを追加してポッドのラベルが一致した場合、既存のReplicaSetをアップグレードするのではなく、新しいReplicaSetが作成されることを意味する
- ロールアウトについて
  - Deploymentコントローラは、特定の戦略を使用して更新プロセスを自動化し、デプロイメントの spec.template への変更に基づいて宣言された新しい状態をシステムが読み取ることができるようにする
    - Deployment メタデータ フィールドのラベルを変更し、マニフェストを再適用しても更新がトリガーされない
  - KubernetesのDeploymentはrollingUpdateとrecreateの2つのストラテジーをサポートしており、前者がデフォルトとなっている。
    - ローリングアップデートが指定された場合、デプロイは必要なレプリカの数にスケールするために新しいReplicaSetを作成し、古いReplicaSetはmaxUnavailbleとmaxSurgeの特定の値に基づいてゼロまでスケールダウンされる
    - Deploymentコントローラーが更新の履歴を保持し、CLIを通じてデプロイメントを以前のバージョンにロールバックできる
  - recreate戦略ははReplicaSet内のPodが完全に停止してもサービスをほとんどダウンさせずに処理できる特定のワークロードに有効な戦略
    - Deploymentコントローラは新しい構成で新しいReplicaSetを作成し、新しいPodをオンラインにする前に以前のReplicaSetを削除する
    - 新しいポッドがオンラインになるのを待っている間はメッセージがキューに入り、新しいポッドがオンラインになるとすぐにメッセージ処理が再開されるため、キューベースのシステムの背後にあるサービスは、このタイプの中断に対応できる
- ベストプラクティスなyamlの一例
  
  ```yaml
  # Web Deployment
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: gb-web-deploy
    labels:
      app: guest-book
      appver: 1.6.9
      environment: production
      release: guest-book-stable
      release number: 34e57f01
  spec:
    strategy:
      type: rollingUpdate
      rollingUpdate:
        maxUnavailbale: 3
        maxSurge: 2
    selector:
      matchLabels:
        app: gb-web
        ver: 1.5.8
      matchExpressions:
        - {key: environment, operator: In, values: [production]}
    template:
      metadata:
        labels:
          app: gb-web
          ver: 1.5.8
          environment: production
      spec:
        containers:
        - name: gb-web-cont
          image: evillgenius/gb-web:v1.5.5
          env:
          - name: GB_DB_HOST
            value: gb-mysql
          - name: GB_DB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: mysql-pass
                key: password
          resources:
            limits:
              memory: "128Mi"
              cpu: "500m"
          ports:
          - containerPort: 80
  ---
  # DB Deployment
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: gb-mysql
    labels:
      app: guest-book
      appver: 1.6.9
      environment: production
      release: guest-book-stable
      release number: 34e57f01
  spec:
    selector:
      matchLabels:
        app: gb-db
        tier: backend
    strategy:
      type: Recreate
    template:
      metadata:
        labels:
          app: gb-db
          tier: backend
          ver: 1.5.9
          environment: production
      spec:
        containers:
        - image: mysql:5.6
          name: mysql
          env:
          - name: MYSQL_PASSWORD
            valueFrom:
              secretKeyRef:
                name: mysql-pass
                key: password
          ports:
          - containerPort: 3306
            name: mysql
          volumeMounts:
          - name: mysql-persistent-storage
            mountPath: /var/lib/mysql
        volumes:
        - name: mysql-persistent-storage
          persistentVolumeClaim:
            claimName: mysql-pv-claim
  ---
  # DB Backup Job
  apiVersion: batch/v1
  kind: Job
  metadata:
    name: db-backup
    labels:
      app: guest-book
      appver: 1.6.9
      environment: production
      release: guest-book-stable
      release number: 34e57f01
    annotations:
      "helm.sh/hook": pre-upgrade
      "helm.sh/hook": pre-delete
      "helm.sh/hook": pre-rollback
      "helm.sh/hook-delete-policy": hook-succeeded
  spec:
    template:
      metadata:
        labels:
          app: gb-db-backup
          tier: backend
          ver: 1.6.1
          environment: production
      spec:
        containers:
        - name: mysqldump
          image: evillgenius/mysqldump:v1
          env:
          - name: DB_NAME
            value: gbdb1
          - name: GB_DB_HOST
            value: gb-mysql
          - name: GB_DB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: mysql-pass
                key: password
          volumeMounts:
            - mountPath: /mysqldump
              name: mysqldump
        volumes:
          - name: mysqldump
            hostPath:
              path: /home/bck/mysqldump
        restartPolicy: Never
    backoffLimit: 3
  ```

  - 間違っているように見えるが（バージョンが異なるところとか）それは↓のベストプラクティスにて解説
- バージョニング、リリース、ロールアウトのベストプラクティス
  - アプリケーション全体を構成するコンテナのバージョンやポッドのデプロイのバージョンとは異なる、アプリケーション全体に対するセマンティック・バージョニングを使用する。
    - これにより、アプリケーションを構成するコンテナと、アプリケーション全体のライフサイクルを独立させることができます。
    - これは、最初はかなり混乱するかもしれないが、一方が他方を変更するタイミングについて原則的な階層的アプローチを取れば、簡単に追跡することができる。
    - 前の例では、コンテナ自体は現在 v1.5.5 だが、ポッドのspecは 1.5.8。これは、新しい ConfigMap、追加の秘密、またはレプリカ値の更新など、ポッドの仕様に変更が加えられたが、使用される特定のコンテナのバージョンは変更されていないことを意味する可能性がある。
    - アプリケーション自体、つまりゲストブックのアプリケーション全体とそのすべてのサービスのバージョンは 1.6.9 で、これは運用によって、この特定のサービスだけでなく、アプリケーション全体を構成する他のサービスなどにも変更が加えられたことを意味する可能性がある。
  - CI/CDパイプラインからのリリースを追跡するために、デプロイメントメタデータにリリースとリリースバージョン/番号のラベルを使用する。
    - リリース名とリリース番号は、CI/CDツールの記録における実際のリリースと一致させる必要がある。
    - これにより、CI/CDプロセスからクラスタへのトレーサビリティが可能になり、ロールバックの識別が容易になる。
    - 前の例では、リリース番号はマニフェストを作成した CD パイプラインのリリース ID に直接由来している。
  - Kubernetesにデプロイするサービスのパッケージ化にHelmを使用する場合、ロールバックやアップグレードが必要なサービスを同じHelmチャートにまとめておくよう、特に注意する。
    - Helmを使えば、アプリケーションの全コンポーネントを簡単にロールバックして、アップグレード前の状態に戻すことができる。
    - Helm はフラット化された YAML 設定を渡す前に、テンプレートとすべての Helm ディレクティブを実際に処理するため、ライフサイクルフックを使用すると、特定のテンプレートの適用を適切に順序付けることができる。
    - オペレータは適切な Helm ライフサイクルフックを使用することで、アップグレードやロールバックが正しく行われるようにすることができる。
    - Helm ライフサイクルフックを使って、Helm リリースのロールバック、アップグレード、削除の前にテンプレートがデータベースのバックアップを実行することを保証できる。
    - また、KubernetesでTTL Controllerがアルファ版から出るまでは、手動でクリーンアップする必要があったJobの実行が成功した後に、Jobが削除されることを保証できる。
    - 組織の運用テンポに合ったリリース命名法に合意する。単純なstable、canary、alphaの状態は、ほとんどの状況において非常に適切である。

## Chapter 7. Worldwide Application Distribution and Staging

- ちょっと飛ばす

## Chapter 8. Resource Management

- Kubernetes Schedulerについて
  - コントロールプレーンでホストされる主要コンポーネントの1つ
  - KubernetesはクラスタにデプロイされたPodの配置を決定することができる
  - Predicate(断定と訳せばよい？←true/falseを返す関数)
    - Kubernetes Schedulerはまずpredicate functionを実行する
      - falseだったら対象から外す
    - チェックするのは以下の通り
      - CheckNodeConditionPred
      - CheckNodeUnschedulablePred
      - GeneralPred
      - HostNamePred
      - PodFitsHostPortsPred
      - MatchNodeSelectorPred
      - PodFitsResourcesPred
      - NoDiskConflictPred
      - PodToleratesNodeTaintsPred
      - PodToleratesNodeNoExecuteTaintsPred
      - CheckNodeLabelPresencePred
      - CheckServiceAffinityPred
  - Priorities(優先順位)
    - スコアを出してランクづけをする
    - チェックするのは以下の通り
      - EqualPriority
      - MostRequestedPriority
      - RequestedToCapacityRatioPriority
      - SelectorSpreadPriority
      - ServiceSpreadingPriority
      - InterPodAffinityPriority
      - LeastRequestedPriority
      - BalancedResourceAllocation
      - NodePreferAvoidPodsPriority
      - NodeAffinityPriority
      - TaintTolerationPriority
      - ImageLocalityPriority
      - ResourceLimitsPriority
- スケジューリングのテクニック
  - 可用性を高めるために同じReplicaSetからのPodをノード間で分散させようとし、リソース使用率のバランスを取ることができる
  - ゾーン障害によるアプリケーションのダウンタイムを軽減するために、アベイラビリティゾーンをまたいでPodをスケジュールすることができる
  - パフォーマンス上のメリットを得るために、特定のホストにPodをコロケートすることもできる
- Pod Affinity と Anti-Affinity について
  - スケジューリングの動作を変更し、スケジューラーの配置決定を上書きすることができる
    - 例えばリソースで優先順位が決まると同じノード、AZに。。とかもあり得る
  - affinity: ポッドを同じノードにスケジュールする
  - anti affinity: ポッドが同じノードにスケジュールされないようにする
  - 以下の例では単一ノードにレプリカをコロケートしないように設定している

    ```yaml
    apiVersion: apps/v1
      kind: Deployment
      metadata:
      name: nginx
      spec:
      selector:
          matchLabels:
          app: frontend
      replicas: 4
      template:
          metadata:
          labels:
              app: frontend
          spec:
          affinity:
              podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchExpressions:
                  - key: app
                      operator: In
                      values:
                      - frontend
                  topologyKey: "kubernetes.io/hostname"
          containers:
          - name: nginx
              image: nginx:alpine
    ```

- nodeSelectorについて
  - 特定のノードに Pod をスケジュールする最も簡単な方法
  - GPUマシンへの割り当てとかそういうのに利用できる

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: redis
    labels:
      env: prod
  spec:
    containers:
    - name: frontend
      image: nginx:alpine
      imagePullPolicy: IfNotPresent
    nodeSelector:
      disktype: ssd
  ```

- Taints と Tolerations(taint(汚染)をtolerations(容認)できるならscheduleする)
  - taintは、ノード上でpodをスケジュールからはじくために使用される。それってanti affinityとおなじじゃね？
    - 同じだけど、異なるユースケースを提供する
    - 特定のパフォーマンス・プロファイルを必要とするPodがあり、特定のノードに他のPodをスケジュールしたくないとする。taintはtolerationと連動しており、taintされたノードをオーバーライドすることができる
    - これはanti affinityより細かい制御ができる
      - 特殊なノードハードウェア
      - 専用ノードリソース
      - デグレードノードの回避
  - コンテナのスケジューリングと実行に影響を与えるテイントの種類は複数ある
    - NoSchedule: そのノードでのスケジューリングを妨げるハードtaint
    - PreferNoSchedule: 他のノードでPodがスケジュールできない場合のみ、スケジューリングする
    - NoExecute: ノード上で既に実行されているポッドを退避させる
    - NodeCondition: 特定の条件を満たした場合にノードをtaintする
  - ディスクドライブの不良でノードが不健康になった場合、taintベースのevictionによって、そのホスト上のpodをクラスタ内の別の健康なノードにリスケジュールすることができる
- Podリソースマネジメントについて
  - Kubernetesでアプリケーションを管理する上で最も重要なことの1つは、Podリソースを適切に管理すること
  - リソースは、コンテナレベルおよびネームスペースレベルで管理することができる
  - ネットワークやストレージなどのリソースがあるが、Kubernetesにはまだそれらのリソースに対する要求や制限を設定する方法はない
  - resouce requestについて
    - コンテナがスケジュールされるためにX量のCPUまたはメモリを必要とすることを定義する
    - ポッドがスケジュールできない場合、必要なリソースが利用できるようになるまでpending状態になる
  - resouce limitと podのQoSについて
    - Podに与えられるCPUやメモリの最大値を定義するもの
    - CPUとメモリに制限を指定すると、指定された制限に達したときにそれぞれ異なるアクションが実行される
      - CPU: コンテナは指定された制限値を超えて使用しないようにスロットルされる
      - メモリ: 制限に達するとポッドが再起動される
    - podは、クラスタ内の同じホストまたは別のホストで再起動される場合がある
    - コンテナに対してlimitを指定することは、アプリケーションがクラスタ内でリソースを公平に割り当てられるようにするための良い方法
    - Podが作成されるとQoSが割り当てられる
      - Guaranteed: request = limitの場合
      - Burstable: request < limitの場合
      - Best effort: requetもしくはlimitを指定しなかった場合
    - 全てのコンテナにrequestとlimitを割り当てることが重要
- PodDisruptionBudgetsについて
  - ある時点で、KubernetesはホストからPodを立ち退かせる(evictする)必要が出てくるかもしれない
  - 2つのevictionの方法がある→任意(voluntary)と不本意(involuntary)
    - voluntary: クラスタのメンテナンスの実行、Cluster Autoscalerのノード割り当て解除、またはPodテンプレートの更新によって引き起こされる可能性がある
    - involuntary: ハードウェア障害、ネットワークパーティション、カーネルパニック、またはノードがリソース不足になることで発生する可能性がある
  - アプリケーションへの影響を最小限に抑えるために、PodDisruptionBudgetを設定して、Podの立ち退きが必要なときにアプリケーションのアップタイムを確保することができる
    - 自発的な立ち退きイベントの際に利用可能な最小ポッド数と利用不可能な最大ポッド数に関するポリシーを設定できる
    - 自発的な立ち退きの例としては、ノードのメンテナンスを行うためにノードを排出する場合などがある
    - たとえば、アプリケーションに属するPodのうち20%以上は一度に停止できないように指定することができる。
    - このポリシーは、常に利用可能でなければならないレプリカの数Xという形で指定することもできる。
  - Minimum availableの例。

    ```yaml
    kind: PodDisruptionBudget
    metadata:
    name: frontend-pdb
    spec:
    minAvailable: 5
    selector:
        matchLabels:
        app: frontend 
    ```

    - 常に5つのレプリカポッドが利用可能でなければならないことを指定している。
    - このシナリオでは、5つのPodが利用可能である限り、立ち退きは望むだけ行うことができる。
  - Maximum unavalableの例

    ```yaml
    apiVersion: policy/v1beta1
    kind: PodDisruptionBudget
    metadata:
    name: frontend-pdb
    spec:
    maxUnavailable: 20%
    selector:
        matchLabels:
        app: frontend
    ```

    - 任意の時点で20%以上のレプリカpodが利用できなくなるように指定している
    - maxUnavailableを50％に指定した場合、それが3つのPodなのか4つのPodなのかがはっきりしないことがある。この場合、Kubernetesは最も近い整数に切り上げるので、maxAvailableは4ポッドとなる。
- namespaceを使ったリソースマネジメント
  - ソフトなマルチテナンシー機能を備えているため、特定のインフラをチームやアプリケーションに割り当てることなく、クラスタ内のワークロードを分離することができる
    - クラスタリソースを最大限に活用しながら、論理的な分離を維持することができる
    - たとえば、チームごとにネームスペースを作成し、各チームにCPUやメモリなどの利用可能なリソース数のクォータを与えることができる
    - 1つのクラスタを使用するチームが複数ある場合は、通常、各チームにネームスペースを割り当てるのが最適
    - クラスターを1つのチームだけに割り当てる場合は、クラスターにデプロイされた各サービスにネームスペースを割り当てるのが理にかなっている場合がある
    - 唯一の解決策はなく、チームの組織と責任によって設計が推進される
  - デフォルトのnamespace
    - kube-system: coredns、kube-proxy、metrics-serverなど、Kubernetesの内部コンポーネントがここにデプロイされる
    - default: リソースオブジェクトでネームスペースを指定しない場合に使用されるデフォルトのネームスペース
    - kube-public: 匿名および未認証のコンテンツに使用され、システム使用用に予約されている
  - 複数のクラスタやnamespaceの切り替えはkubensやkubectxが便利
  - ResouceQuotaについて
    - 複数のチームやアプリケーションが1つのクラスタを共有する場合、ネームスペースにResourceQuotasを設定することが重要
    - クラスタを論理的な単位で分割して、単一のネームスペースがクラスタのリソースのシェアを超える量を消費できないようにすることができる
    - 以下のリソースにクォータを設定することができる
      - Compute Resouces
        - requests.cpu: CPU要求の合計がこの値を超えることはできない
        - limits.cpu: CPUの制限値の合計がこの値を超えることはできない
        - requests.memory: メモリ要求の合計は、この量を超えることはできない
        - limit.memory: メモリの制限の合計は、この量を超えることはできない
      - Storage Resouces
        - requests.storage: ストレージの要求の合計はこの値を超えることはできない
        - persistentvolumeclaims: ネームスペースに存在できるPersistentVolumeのクレームの総数
        - storageclass.request: 指定されたストレージクラスに関連するボリューム要求は、この値を超えることはできない
        - storageclass.pvc: ネームスペースに存在できるPersistentVolumeのクレームの総数
      - Object count quotas (only an example set)  
        - count/pvc
        - count/services
        - count/deployments
        - count/replicasets
    - 例

      ```yaml
      apiVersion: v1
      kind: ResourceQuota
      metadata:
        name: mem-cpu-demo
        namespace: team-1
      spec:
        hard:
          requests.cpu: "1"
          requests.memory: 1Gi
          limits.cpu: "2"
          limits.memory: 2Gi
          persistentvolumeclaims: "5"
          requests.storage: "10Gi
      ```

  - LimitRangeについて
    - Kubernetesにはアドミッション・コントローラが用意されており、specに何も記載がない場合は自動的にlimit, requestを設定することができる
    - 例

      ```yaml
      apiVersion: v1
      kind: LimitRange
      metadata:
        name: team-1-limit-range
      spec:
        limits:
        - default:
            memory: 512Mi
          defaultRequest:
            memory: 256Mi
          type: Container
      ```

  - Cluster Scalingについて
    - クラスターをデプロイする際に最初に決定しなければならないことのひとつが、クラスター内で使用するインスタンスサイズ
      - 特にひとつのクラスターで複数のワークロードを混在させる場合は難しい
      - CPUとメモリのバランスをうまくとることもひとつの方法
    - Manual Scalingについて
      - マネージドKubernetesのようなツールを使用している場合、クラスタを簡単にスケーリングすることができる
      - これらのツールでは、ノードプールを作成することもでき、すでに稼働しているクラスターに新しいインスタンスタイプを追加することができる
        - あるワークロードはCPU駆動型で、他のワークロードはメモリ駆動型のアプリケーションである場合がある
        - ノードプールを利用すれば、1つのクラスタ内で複数のインスタンスタイプを混在させることができる
      - クラスタのオートスケールには考慮すべき点があり、ほとんどのユーザーは、リソースが必要になったときに手動でノードをプロアクティブにスケーリングすることから始めた方がよい
    - Cluster autoscalingについて
      - クラスタで利用可能な最小ノードと、クラスタがスケールできる最大ノード数を設定することが可能
      - Cluster Autoscalerは、Podがペンディングになるタイミングを基準にスケールを決定する
        - Kubernetesスケジューラが4,000 Mibのメモリ要求を持つPodをスケジュールしようとし、クラスタが2,000 Mibしか利用できない場合、そのPodはペンディング状態になる。
        - Podがペンディング状態になった後、Cluster Autoscalerはクラスタにノードを追加する
      - Cluster Autoscalerの欠点
        - 新しいノードが追加されるのはPodがペンディングになる前なので、ワークロードがスケジュールされたときに新しいノードがオンラインになるのを待つことになる可能性がある
      - Cluster Autoscalerは、リソースが不要になった後に、クラスタのサイズを縮小することもできる
      - リソースが不要になると、ノードを排出し、クラスタ内の新しいノードにPodを再スケジュールする
      - PodDisruptionBudgetを使用して、クラスタからノードを削除するためにDrain操作を実行するときに、アプリケーションに悪影響を与えないようにすることが望まれる
  - Application Scalingについて
    - Deployment内のレプリカの数を手動で変更することで、アプリケーションをスケールすることができる
    - ReplicasetやReplicationControllerの利用はNG
    - 手動スケーリングは、静的なワークロードやワークロードが急増する時期がわかっている場合には全く問題ないが、突然のスパイクが発生するワークロードや静的ではないワークロードの場合、手動スケーリングはアプリケーションにとって理想的ではない
    - HPA（Horizontal Pod Autoscaler）を使いましょう
      - CPU、メモリ、またはカスタムメトリクスに基づいてデプロイメントをスケーリングすることができる
      - 利用可能なポッドの最小数と最大数を設定することも可能
        - たとえば、Podの最小数を3、最大数を10に設定し、デプロイがCPU使用率80%に達するとスケールするHPAポリシーを定義できる。
        - アプリケーションのバグや問題によって、HPAがレプリカを無限にスケールさせることは避けたいため、最小値と最大値の設定は非常に重要。
      - HPAには、メトリックの同期、レプリカのアップスケール、ダウンスケールに関する次のデフォルト設定がある。
      - ワークロードが極端に変動する場合は、設定を弄って特定のユースケースに最適化する価値がある
      - カスタムメトリクスを使うのはとてもよい
        - たとえば、外部のストレージキューで収集しているメトリクスに基づいてスケーリングすることができる
    - Vertical Pod Autoscaler（VPA）もあるけど非推奨
      - アーキテクチャ上、スケールアウトできないワークロードの場合、リソースを自動的にスケールさせるのに効果的
        - 例えばDB(できればマネージドのを使え。。)
      - Kubernetes v1.15では、VPAはプロダクションデプロイメントに推奨されない
- リソース管理ベストプラクティス
  - Podのanti affinityを利用して、複数のアベイラビリティゾーンにワークロードを分散させ、アプリケーションの高可用性を確保する
  - GPU 対応ノードなどの特殊なハードウェアを使用している場合は、taintを使用して、GPU を必要とするワークロードのみがこれらのノードにスケジューリングされるようにする。
  - NodeConditionテイントを使用して、障害ノードやデグレードノードをプロアクティブに回避する。
  - podのspecにnodeSelectorを適用して、クラスタに配置した特殊なハードウェアにポッドをスケジュールする。
  - 実運用に入る前に、さまざまなノード・サイズを実験して、ノード・タイプのコストと性能の良い組み合わせを見つける。
  - 性能特性の異なるワークロードを混在して展開する場合は、ノードプールを利用して、1つのクラスターに複数のノードタイプを混在させる。
  - クラスタにデプロイされたすべてのpodに対して、メモリとCPUの制限を設定することを確認する。
  - ResourceQuotasを使用して、複数のチームまたはアプリケーションにクラスタ内のリソースを公平に割り当てるようにする。
  - LimitRangeを実装して、制限や要求を設定しないPodの仕様に対してデフォルトの制限と要求を設定する。
  - Kubernetesでのワークロードプロファイルを理解するまで、手動によるクラスタースケーリングから始める。オートスケールを使用することもできるが、ノードのスピンアップタイムとクラスタのスケールダウンに関する追加の考慮事項が伴う。
  - 変動が激しく、使用量が予想外に急増するワークロードには、HPAを使用します。

## Chapter 9. Networking, Network Security, and Service Mesh

- Kubernetesは事実上、接続されたシステムのクラスタにまたがる分散システムのマネージャー→接続されたシステム同士がどのように通信するかが即座に非常に重要になり、その鍵を握るのがネットワーク
- Kubernetesネットワークの原理
  - 同じPod内のすべてのコンテナは、同じネットワーク空間を共有する。これにより、コンテナ間のローカルホスト通信が効果的に行えるようになる
  - すべてのpodは、ネットワーク・アドレス変換（NAT）なしで互いに通信する必要がある
    - ノードがNATを介さずにポッドと直接通信できることにも及ぶ。
    - これにより、ホストベースのエージェントやシステム・デーモンが必要に応じてポッドに通信することができる
  - KubernetesのServiceは、各ノードで見つかる耐久性のあるIPアドレスとポートを表し、そのサービスにマッピングされたエンドポイントにすべてのトラフィックを転送する
- ネットワークプラグインについて
  - kubenet
    - Kubernetesにデフォルトで搭載されている最も基本的なネットワークプラグイン
    - ベストプラクティス
      - Kubenetはシンプルなネットワークスタックを可能にし、すでに混雑しているネットワークで貴重なIPアドレスを消費しないようにする。これは特に、オンプレミスのデータセンターに拡張されるクラウドネットワークに当てはまる。
      - podのCIDR範囲が、クラスタの潜在的なサイズと各クラスタ内のポッドを処理するのに十分な大きさであることを確認する。kubeletで設定されているノードあたりのポッドのデフォルトは110だが、これを調整することができる。
      - トラフィックが適切なノードのポッドを見つけることができるように、ルートルールを理解し、それに応じて計画する。クラウドプロバイダーでは通常自動化されているが、オンプレミスやエッジケースでは自動化としっかりとしたネットワーク管理が必要になる。
  - CNI
    - CNI が提供するインタフェースと最小限の API アクション、そしてクラスタで使用されるコンテナランタイムとのインタフェースを規定している
    - ベストプラクティス
      - インフラストラクチャの全体的なネットワーク目標を達成するために必要な機能セットを評価する。
        - CNIプラグインの中には、ネイティブの高可用性、マルチクラウド接続、Kubernetesネットワークポリシーのサポート、その他様々な機能を提供する。
      - パブリッククラウドプロバイダー経由でクラスタを実行する場合、クラウドプロバイダーのソフトウェア定義ネットワーク（SDN）にネイティブでないCNIプラグインが実際にサポートされているかどうかを確認する。
      - ネットワークセキュリティツール、ネットワーク観測性、管理ツールが、選択した CNI プラグインと互換性があるかどうかを確認し、互換性がない場合は、既存のツールを置き換えることができるツールを調査する。
        - Kubernetesのような大規模分散システムに移行するとニーズが拡大するため、観測性とセキュリティの両機能を失わないことが重要である。
        - Weaveworks Weave Scope、Dynatrace、SysdigといったツールをどのKubernetes環境にも追加でき、それぞれにメリットがある。
        - Azure AKS、Google GCE、AWS EKSなどのクラウドプロバイダーのマネージドサービスで実行している場合は、Azure Container InsightsやNetwork Watcher、Google Stackdriver、AWS CloudWatchなどのネイティブツールを探す。
        - どのツールを使うにしても、少なくともネットワークスタックと、素晴らしいGoogle SREチームとRob Ewashuckによって有名になった「4つのゴールデンシグナル（レイテンシ、トラフィック、エラー、飽和）」についての洞察を提供する必要がある。
      - SDN ネットワーク空間とは別のオーバーレイネットワークを提供しない CNI を使っている場合、ノード IP、ポッド IP、内部ロードバランサ、クラスタのアップグレードとスケールアウト処理のオーバーヘッドを扱うための適切なネットワークアドレス空間を持っていることを確認する。
- Services
  - KubernetesクラスタにPodをデプロイすると、Kubernetesのネットワークの基本ルールと、そのルールを促進するために使用されるネットワークプラグインのために、Podは同じクラスタ内の他のPodとしか直接通信できない
  - CNIプラグインの中には、ノードと同じネットワーク空間上のポッドにIPを与えるものがあり、技術的にはポッドのIPが判明した後は、クラスタ外から直接アクセスできるようになる(AWSもそう)
  - KubernetesのPodはエフェメラルであるため、Podが提供するサービスにアクセスする効率的な方法ではない
  - Services APIは、Kubernetesクラスタ内で耐久性のあるIPとポートを割り当て、サービスのエンドポイントとして適切なポッドに自動的にマッピングすることを可能にする
    - kube-proxyが管理している
  - Servicesオブジェクトを定義する際には、serviceの種類を定義する必要があります。serviceタイプによって、エンドポイントをクラスタ内だけで公開するか、クラスタ外で公開するかが決まる
  - serviceタイプは4つある
    - Cluster IP
      - デフォルトのserviceタイプ
      - 指定されたserviceのCIDR範囲からIPが割り当てられる
      - selectorフィールドを使ってバックエンドのPodにIPとポート、プロトコルのマッピングを提供する(selectorを持たないこともできる)
      - DNSパターンは`<service_name>.<namespace_name>.svc.cluster.local`となる。同じnamespaceの場合は`<service_name>`だけで名前解決できる
    - NodePort
      - クラスタの各ノード上の上位ポートを各ノード上のサービスIPとポートに割り当る。
      - 高レベルのNodePortは30,000から32,767の範囲にあり、静的に割り当てるか、サービス仕様で明示的に定義することができる
    - ExternalName
      - 実際にはほとんど使用されない
      - クラスタ耐久性のあるDNS名を外部DNSネームドサービスに渡すのに便利
        - たとえば、awsのAuroraのDNS名って長いけど、それを短くしたいときとか。。
    - LoadBalancer
      - クラウドプロバイダや他のプログラマブルなクラウドインフラストラクチャサービスとの自動化を可能にする
      - 各クラウドプロバイダには、内部専用のロードバランサーやAWS ELB設定パラメータなど、他の機能を有効にするための固有のアノテーションがいくつかある
      - 内部的には前段にLoadBalancerを置いてNodePortを利用している
- Ingress / Ingress Controller
  - アプリケーションのロードバランサーとしてはHTTPを利用することが多い→Ingressを利用する
  - TLS終端, WAFなどを利用できる
- ServiceとIngressのベストプラクティス
  - アプリケーションが相互に接続された複雑な仮想ネットワーク環境を構築するには、綿密な計画が必要
  - アプリケーションのさまざまなサービスが互いに、また外部と通信する方法を効果的に管理するには、アプリケーションが変更されるたびに常に注意を払う必要がある
  - クラスタの外部からアクセスする必要があるサービスの数を制限する。理想的には、ほとんどのサービスがClusterIPであり、外部向けのサービスだけがクラスタの外部に公開されるようにする
  - もし公開する必要があるサービスが主にHTTP/HTTPSベースのサービスであれば、Ingress APIとIngressコントローラを使用して、TLSターミネーションでバッキングサービスにトラフィックをルーティングするのがベスト。
    - 使用するIngressコントローラの種類によっては、レート制限、ヘッダー書き換え、OAuth認証、観測性などの機能を、アプリケーション自体に組み込むことなく利用できるようになる。
  - Web ベースのワークロードを安全に取り込むために必要な機能を持つ Ingress コントローラを選択する。
    - 特定の構成アノテーションの多くが実装間で異なり、エンタープライズKubernetes実装間でデプロイコードを移植することができないため、1つに標準化して企業全体で使用するようにする
  - クラウドサービスプロバイダ固有のIngressコントローラオプションを評価し、インフラ管理とIngressの負荷をクラスタの外に移動させ、かつKubernetes API構成を可能にする。
  - 主にAPIを外部に提供する場合、APIベースのワークロードをより細かく調整できるKongやAmbassadorなど、API専用のIngressコントローラを評価する。
    - NGINXやTraefikなどもAPIチューニングを提供するかもしれないが、特定のAPIプロキシシステムほどきめ細かくはないだろう。
  - IngressコントローラをKubernetesのPodベースのワークロードとしてデプロイする場合、デプロイが高可用性と集約的なパフォーマンススループットのために設計されていることを確認すること。
    - メトリクスの観測性を利用してIngressを適切にスケールさせるが、ワークロードのスケール中にクライアントが中断しないよう、十分なクッションを入れる。
- Network Security Policyについて
  - NetworkPolicy APIは、ワークロードで定義されたネットワークレベルの入口と出口のアクセス制御を可能にする
  - 書くpolicyのspecにはpodSelector、ingress、egress、およびpolicyTypeの各フィールドがある
  - NetworkPolicyオブジェクトはnamespaceオブジェクトであるため、PodSelectorにselectorが与えられない場合、namespace内のすべてのpodがpolicyのスコープに該当する
    - ingress/egressルールが定義されている場合、ポッドとの間で許可されるもののホワイトリストが作成される
    - つまりingressフィールドを空にするとingressに対してdeny-all、egressフィールドを空にするとegressに対してdeny-all
    - ポートリストとプロトコルリストもサポートされており、許可する通信の種類をさらに絞り込むことができる
  - web, api(app), dbの3階層のネットワークポリシーの例
    - Default deny rule:

        ```yaml
        apiVersion: networking.k8s.io/v1
        kind: NetworkPolicy
        metadata:
          name: default-deny-all
        spec:
          podSelector: {}
          policyTypes:
          - Ingress
        ```

    - Web layer network policy:

      ```yaml
      apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      metadata:
        name: webaccess
      spec:
        podSelector:
          matchLabels:
            tier: "web"
        policyTypes:
        - Ingress
        ingress:
        - {} # なんでも許可
      ```

    - API layer network policy:

      ```yaml
      apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      metadata:
        name: allow-api-access
      spec:
        podSelector:
          matchLabels:
            tier: "api"
        policyTypes:
          - Ingress
        ingress:
          - from:
            - podSelector:
                matchLabels:
                  tier: "web" # webからのみ許可
      ```

    - Database layer network policy:

      ```yaml
      apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      metadata:
        name: allow-db-access
      spec:
        podSelector:
          matchLabels:
            tier: "db"
        policyTypes:
        - Ingress
        ingress:
        - from:
          - podSelector:
              matchLabels:
                tier: "api"  # apiからのみ許可
      ```

- Network Policyのベストプラクティス
  - 最初はポッドへのトラフィックのingressに焦点を当てる。
    - ingressとegressのルールで問題を複雑にすると、ネットワークのトレースが悪夢のようになる。
    - トラフィックが予想通りに流れるようになったら、機密性の高いワークロードへのフローをさらに制御するために、egressルールを検討し始めるとよい
    - また、ingressルールリストに何も入力されていなくても、多くのオプションがデフォルトで設定されるため、ingressが有利です。
  - 使用するネットワーク・プラグインが、NetworkPolicy API に対する何らかの独自のインタフェースを持つか、他のよく知られたプラグインをサポートしていることを確認する。
    - プラグインの例としては、Calico、Cilium、Kube-router、Romana、Weave Netなどがある
  - ネットワークチームが「デフォルトの拒否」ポリシーに慣れている場合、保護すべきワークロードを含むクラスタの各ネームスペースに対して、次のようなネットワークポリシーを作成する。

    ```yaml
    apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    metadata:
      name: default-deny-all
    spec:
      podSelector: {}
      policyTypes:
      - Ingress
    ```

    - これにより、別のネットワーク・ポリシーが削除された場合でも、ポッドが誤って「公開」されることがなくなる。
  - インターネットからアクセスする必要があるpodがある場合、ラベルを使用して、イングレスを許可するネットワーク・ポリシーを明示的に適用する。
    - パケットの実際の送信元IPがインターネットではなく、ロードバランサーやファイアウォールなどのネットワークデバイスの内部IPである場合に備えて、フロー全体を意識する。
    - たとえば、allow-internet=trueラベルを持つポッドに対して、すべての（外部を含む）ソースからのトラフィックを許可するには、次のようにする

      ```yaml
      apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      metadata:
        name: internet-access
      spec:
        podSelector:
          matchLabels:
            allow-internet: "true"
        policyTypes:
        - Ingress
        ingress:
        - {}
      ```
  
  - ルール自体はネームスペースに依存するため、ルールを作成しやすいように、アプリケーションのワークロードを単一のネームスペースに揃えるようにする。
    - ネームスペースをまたぐ通信が必要な場合は、可能な限り明示し、特定のラベルを使用してフローパターンを識別できるようにする。

    ```yaml
    apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    metadata:
      name: namespace-foo-2-namespace-bar
      namespace: bar
    spec:
      podSelector:
        matchLabels:
          app: bar-app
      policyTypes:
      - Ingress
      ingress:
      - from:
        - namespaceSelector:
            matchLabels:
              networking/namespace: foo
          podSelector:
            matchLabels:
              app: foo-app
    ```
  
  - 必要な正しいトラフィックパターンを調査する時間を確保するために、制限の少ないポリシーを持つテストベッドのnamespaceを用意します。
- Service Meshについて
  - 何千ものエンドポイント間でロードバランスをとり、互いに通信し、外部リソースにアクセスし、外部ソースからアクセスされる可能性のある何百ものサービスをホストする単一クラスターを想像するのは簡単だけど、サービス間の接続をすべて管理、保護、監視、追跡しようとすると、特にシステム全体から出入りするエンドポイントの動的な性質を考えると、かなり困難な作業となるよね。。
  - 専用のデートプレーンとコントロールプレーンを使って、これらのサービスの接続方法とセキュリティの確保を制御できるようにする
  - サービスメッシュは以下を提供する
    - メッシュ全体に分散されたきめ細かいトラフィック・シェーピング・ポリシーによるトラフィックのロードバランシング
    - メッシュのメンバーであるサービス（クラスタ内または別のクラスタ内のサービス、またはメッシュのメンバーである外部システムを含む）のサービス・ディスカバリー
    - OpenTracing標準に準拠したJaegerやZipkinのようなトレースシステムを使った分散サービス全体のトレースを含む、トラフィックとサービスの観測性
    - 相互認証によるメッシュ内のトラフィックのセキュリティ。場合によっては、ポッド間や東西のトラフィックだけでなく、南北のセキュリティと制御を提供するIngressコントローラも提供される
    - サーキットブレーカー、リトライ、デッドラインなどのパターンを可能にする回復力、健全性、および障害防止機能
  - これらの機能はすべて、メッシュに参加するアプリケーションに統合されており、アプリケーションをほとんど、あるいはまったく変更する必要がない
  - Service Mesh Interface（SMI）として基本的な機能セットの標準的なインターフェースが用意される（CRIとかCNI, CSI, CPIとかと並ぶ位置付け）
- Service Meshのベストプラクティス
  - サービスメッシュが提供する主要な機能の重要性を評価し、最も重要な機能を最小限のオーバーヘッドで提供する現在のサービスを決定する。
    - ここでいうオーバーヘッドとは、人間の技術的負債とインフラの資源的負債の両方を指す。
    - 本当に必要なのは特定のポッド間の相互 TLS だけなら、それをプラグインに統合して提供する CNI を見つけるのは簡単。
  - マルチクラウドやハイブリッドシナリオのようなクロスシステムメッシュの必要性は重要な要件か。
    - すべてのサービスメッシュがこの機能を提供しているわけではないし、提供していたとしても、複雑なプロセスで環境に脆弱性をもたらすことがよくあります。
  - サービスメッシュの多くはオープンソースのコミュニティベースのプロジェクトであり、環境を管理するチームがサービスメッシュに慣れていない場合は、商業的にサポートされているサービスの方が良い選択肢となる場合がある。
    - Istioをベースにした商用サポートおよびマネージド・サービスメッシュを提供し始めている企業もあり、Istioが管理が複雑なシステムであることはほぼ共通認識となっているため、これは有用である。

## Chapter 10. Pod and Container Security

- PodSecurityPolicyを効果的に実装することは驚くほど難しく、多くの場合、オフにされるか、他の方法で回避される。
  - しかし、PodSecurityPolicyを完全に理解するために時間をかけることはとてもおすすめ。
  - これは、クラスタ上で実行できるものと特権のレベルを制限することによって、攻撃対象領域を減らすための唯一で最も効果的な手段。
- PodSecurityPolicyの有効化
  - PodSecurityPolicyリソースに定義された条件を実施するために，対応するアドミッション・コントローラを有効にする必要がある
    - ただし、パブリッククラウドプロバイダーやクラスタ運用ツールの間で広く利用されているわけではない
    - PodSecurityPolicyを有効にする際は、最初に十分な準備をしないとワークロードがブロックされる可能性があるため、慎重に進める必要がある。
  - PodSecurityPolicyが有効化されていることの確認方法
    - PodSecurityPolicy APIが有効になっていることを確認する
      - `kubectl get psp`
    - `api-server flag --enable-admission-plugins` で PodSecurityPolicy アドミッション・コントローラを有効にする
  - ワークロードが稼働している既存のクラスタでPodSecurityPolicyを有効にする場合、必要なすべてのポリシー、サービスアカウント、ロール、およびロールバインディングを作成してから、アドミッションコントローラを有効にする必要がある
- PodSecurityの構造
  - 特権的なワークロードを許可する設定の例

    ```yaml
    apiVersion: policy/v1beta1
    kind: PodSecurityPolicy
    metadata:
      name: privileged
    spec:
      privileged: true
      allowPrivilegeEscalation: true
      allowedCapabilities:
        - '*'
      volumes:
        - '*'
      hostNetwork: true
      hostPorts:
        - min: 0
          max: 65535
      hostIPC: true
      hostPID: true
      runAsUser:
        rule: 'RunAsAny'
      seLinux:
        rule: 'RunAsAny'
      supplementalGroups:
        rule: 'RunAsAny'
      fsGroup:
        rule: 'RunAsAny'
    ```

  - 制限付きアクセスの例

    ```yaml
    apiVersion: policy/v1beta1
    kind: PodSecurityPolicy
    metadata:
      name: restricted
    spec:
      privileged: false
      allowPrivilegeEscalation: false
      requiredDropCapabilities:
        - ALL
      volumes:
        - 'configMap'
        - 'emptyDir'
        - 'projected'
        - 'secret'
        - 'downwardAPI'
        - 'persistentVolumeClaim'
      hostNetwork: false
      hostIPC: false
      hostPID: false
      runAsUser:
        rule: 'RunAsAny'
      seLinux:
        rule: 'RunAsAny'
      supplementalGroups:
        rule: 'MustRunAs'
        ranges:
          - min: 1
            max: 65535
      fsGroup:
        rule: 'MustRunAs'
        ranges:
          - min: 1
            max: 65535
      readOnlyRootFilesystem: false
    ```
  
  - RBACにより、これらのポリシーを使用するためのアクセス権をサービスアカウントに付与する必要がある

    ```yaml
    kind: ClusterRole
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
      name: psp-restricted
    rules:
    - apiGroups:
      - extensions
      resources:
      - podsecuritypolicies
      resourceNames:
      - restricted
      - privileged
      verbs:
      - use
    
    ---

    kind: ClusterRoleBinding
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
      name: psp-restricted
    subjects:
    - kind: Group
      name: system:serviceaccounts
      namespace: kube-system
    roleRef:
      kind: ClusterRole
      name: psp-restricted
      apiGroup: rbac.authorization.k8s.io
    ```

- PodSecurityPolicyを進めるために
  - 合理的なデフォルトのポリシーの準備
    - PodSecurityPolicyの本当の力は、クラスタ管理者および/またはユーザーが、ワークロードが一定のセキュリティレベルを満たすことを保証できるようにすること
    - 実際には、多くのワークロードがルートとして実行され、hostPathボリュームを使用し、または他の危険な設定を持っているため、ワークロードを起動して実行するためだけにセキュリティホールのあるポリシーを作成しなければならないことを、しばしば見過ごすことがある
  - 膨大な労力
    - PodSecurityPolicyが有効になっていないKubernetes上ですでに稼働しているワークロードが多数ある場合、ポリシーを正しく作成することは大きな投資が必要
  - 開発者はPodSecurityPolicyを学ぶことに興味があるか？
    - 開発者はPodSecurityPolicyを学びたいと思うか？
    - そうするためのインセンティブは何？
    - PodSecurityPolicyを有効にするために、前もって多くの調整と自動化を行わなければ、PodSecurityPolicyは全く採用されない可能性が非常に高い
  - デバッグが面倒
    - ポリシー評価のトラブルシューティングが難しい(IAMみたいなものか。。)
      - 例えば、ワークロードが特定のポリシーにマッチした、あるいはマッチしなかった理由を理解したいと思うかもしれない
      - それを容易にするツールやログは、現段階では存在しない
  - 自分のコントロール外のアーティファクトに頼ってないか？
    - Docker Hubや他のパブリックリポジトリからイメージを引っ張ってきないか？
    - それらは何らかの形であなたのポリシーに違反している可能性があり、あなたのコントロールの及ばないところで修正することになる
    - もう1つの一般的な場所は、Helmチャート
      - 適切なポリシーが適用された状態で出荷されているか？
- PodSecurityPolicyのベストプラクティス
  - すべてはRBACに帰結する
    - PodSecurityPolicyはRBACによって決定される。
    - この関係によって、現在のRBACポリシー設計のすべての欠点が実際に露呈される。
    - RBACとPodSecurityPolicyの作成と保守を自動化することがどれほど重要であるか、私たちは強調することができない。
    - 特に、サービスアカウントへのアクセスをロックダウンすることは、ポリシーを使用する上で重要なポイント
  - ポリシーの範囲を理解する。
    - クラスタ上でどのようにポリシーを配置するかを決定することは、非常に重要。
    - ポリシーは、クラスタ全体、名前空間、またはワークロードに特化したスコープにすることができる。
    - Kubernetesクラスタ運用の一部であるクラスタには、より寛容なセキュリティ特権を必要とするワークロードが常に存在するため、寛容なポリシーを使用して不要なワークロードを阻止するために適切なRBACがあることを確認する。
  - 既存のクラスターでPodSecurityPolicyを有効にしたいか？
    - [この便利なオープンソースツール](https://github.com/sysdiglabs/kube-psp-advisor)を使用して、現在のリソースに基づいてポリシーを生成するといい。
    - そこから、ポリシーに磨きをかけることができる。
    - PodSecurityPolicyはクラスタを安全に保つことを支援する非常に強力なAPIだが、使用するには高い税金を要求される。
    - 慎重な計画と実用的なアプローチで、PodSecurityPolicyはどんなクラスタにもうまく実装できる。少なくとも、セキュリティチームを満足させることができるはず。
- ワークロードの分離とRuntimeClass
  - コンテナランタイムは、まだ大部分が安全でないワークロード分離の境界と考えられている
  - 現在最も一般的なランタイムが安全であると認められるようになるかどうか、明確な道筋はない
  - 異なるワークロード分離を提供するこれらのコンテナランタイムの導入により、ユーザーは同じクラスタ内で分離保証に基づいて多くの異なるランタイムを選択することができる
    - たとえば、同じクラスタ内で信頼できるワークロードと信頼できないワークロードを、異なるコンテナランタイムで実行させることができる
  - RuntimeClassは、コンテナランタイムの選択を可能にするAPIとしてKubernetesに導入された
    - PodのspecでRuntimeClassNameを使用することで、ワークロードに対して特定のRuntimeClassを定義することができる

      ```yaml
      apiVersion: v1
      kind: Pod
      metadata:
        name: nginx
      spec:
        runtimeClassName: firecracker
      ```

- ランタイムの実装
  - CRI containerd
    - シンプルさ、堅牢性、移植性に重点を置いたコンテナランタイムの API ファサード。
  - cri-o
    - Open Container Initiative (OCI) をベースとした、Kubernetes 用の軽量なコンテナランタイムの専用実装
  - Firecracker
    - KVM（Kernel-based Virtual Machine）上に構築された仮想化技術で、非仮想化環境において、従来のVMのセキュリティと分離性を備えたマイクロVMを非常に高速に起動することができる
  - gVisor
    - OCI互換のサンドボックス型ランタイム。新しいユーザースペースカーネルでコンテナを実行し、低オーバーヘッドで安全かつ分離されたコンテナランタイムを提供する。
  - Kata Containers
    - コンテナのように感じられ、動作する軽量なVMを実行することにより、VMのようなセキュリティと分離を提供する、安全なコンテナランタイムを構築しているコミュニティ。
- ワークロードの分離とRuntimeClassのベストプラクティス
  - RuntimeClassを使用して異なるワークロード分離環境を実装すると、運用環境が複雑になる。
    - つまり、提供する分離の性質上、異なるコンテナランタイム間でワークロードがポータブルでない可能性がある。
    - 異なるランタイム間でサポートされる機能のマトリックスを理解することは複雑であり、ユーザーエクスペリエンスの低下を招く。
    - 可能であれば、混乱を避けるために、それぞれ単一のランタイムを持つ別々のクラスターを持つことをお勧めする
  - ワークロードの分離は、安全なマルチテナンシーを意味しない。
    - セキュアなコンテナランタイムを実装したとしても、KubernetesクラスタとAPIが同じようにセキュアになったとは限らない。
    - Kubernetesの端から端までの総表面積を考慮する必要がる。
    - 分離されたワークロードがあるからといって、それがKubernetes API経由で悪意ある行為者によって変更されないとは限らない。
  - 異なるランタイム間のツールは一貫性がない。
    - デバッグやイントロスペクションのために、コンテナランタイムのツールに依存しているユーザーがいるかもしれない。
    - 異なるランタイムを持つということは、実行中のコンテナを一覧表示するために docker ps を実行できなくなる可能性がある。(dockerdはk8s1.24から非推奨になったよね)
    - これは、トラブルシューティングの際に混乱と複雑化を招きます。
- その他のPodおよびコンテナセキュリティ
  - アドミッション・コントローラ
    - PodSecurityPolicyで深みにはまることを心配している場合、ほんの一部の機能を提供する
      - DenyExecOnPrivilegedやDenyEscalatingExecなどのアドミッション・コントローラをアドミッションWebhookと組み合わせて使用し、SecurityContextワークロード設定を追加して、同様の結果を達成することが可能
      - アドミッションコントロールの詳細については、第17章を参照。
  - 侵入・異常検知ツール
    - コンテナランタイム内でポリシーをイントロスペクトして実施したい場合はどうすればよいか。
    - これを実現するオープンソースツールが存在する。これらのツールは、Linuxシステムコールをリッスンしてフィルタリングするか、Berkeley Packet Filter（BPF）を利用することで動作する。
      - そのようなツールの1つがFalco。
      - FalcoはCloud Native Computing Foundation (CNCF)のプロジェクトで、Demonsetとしてインストールするだけで、実行中にポリシーを設定し、実施することができるようになる。
      - Falcoは一つのアプローチに過ぎない。この分野のツールに目を通し、何が自分に合っているかを確認することをお勧めする。

## Chapter 11. Policy and Governance for Your Cluster

- ポリシーとガバナンスが重要な理由
  - ヘルスケアや金融サービスなど、高度に規制された環境で事業を展開している場合でも、単にクラスタ上で実行されているものを確実に管理したい場合でも、企業で定められたポリシーを実施する方法が必要になる。
  - これらのポリシーが定義された後（組織で）、ポリシーを実装し、これらのポリシーに準拠したクラスタを維持する方法を決定する必要がある。
  - これらのポリシーは、規制コンプライアンスを満たすため、または単にベストプラクティスを実施するために設けられるかもしれません。どのような理由であれ、これらのポリシーを実装する際には、開発者の俊敏性とセルフサービスを犠牲にしないことを確認する必要がある
- ここでいうポリシーとは？
  - Kubernetesでは、ポリシーはどこにでもある。
    - NetworkPolicyであれ、PodSecurityPolicyであれ、私たちは皆、ポリシーとは何か、いつそれを使うべきかを理解するようになった。
      - 私たちは、Kubernetesのリソースspecで宣言されたものが、ポリシー定義通りに実装されることを信頼している。
      - NetworkPolicyもPodSecurityPolicyも、実行時に実装されるもの。
    - しかし、このKubernetesリソース仕様で実際に定義されている内容は、誰が管理しているのか。
      - それは、ポリシーとガバナンスの仕事。
      - 実行時にポリシーを実装するのではなく、ガバナンスの文脈でポリシーといえば、Kubernetesリソース仕様のフィールドや値そのものを制御するpolicyを定義することを意味する。
      - これらのポリシーに対して準拠したKubernetesリソース仕様のみが許可され、クラスタステートにコミットされる。
- Cloud-Native Policy Engine
  - どのリソースが準拠するのかを判断できるようにするためには、さまざまなニーズに柔軟に対応できるポリシーエンジンが必要。
  - Open Policy Agent（OPA）は、オープンソースの柔軟かつ軽量なポリシーエンジンで、クラウドネイティブのエコシステムで人気が高まっている。
  - Gatekeeperが人気
- Gatekeeperの導入
  - Gatekeeperは、クラスターのポリシーとガバナンスのための、オープンソースのカスタマイズ可能なKubernetesのアドミッションウェブフックである。
  - Gatekeeperは、OPA制約フレームワークを活用し、カスタムリソース定義（CRD）ベースのポリシーを適用する。
  - CRDを使用することで、ポリシーのオーサリングと実装を切り離した統合的なKubernetesエクスペリエンスを実現する。
  - ポリシーのテンプレートは制約テンプレートと呼ばれ、クラスタ間で共有・再利用が可能。
  - Gatekeeperは、リソースの検証や監査機能を実現する。
  - Gatekeeperの素晴らしい点は、ポータブルであること
    - どのKubernetesクラスタでも実装できる
    - すでにOPAを使用している場合は、そのポリシーをGatekeeperに移植できるかもしれない。
  - 例えばこんなPolicyを定義できる
    - サービスはインターネット上で一般に公開してはならない。
    - 信頼できるコンテナレジストリからのコンテナのみを許可する。
    - すべてのコンテナには、リソース制限が必要。
    - イングレスのホスト名は重複してはならない。
    - イングレスは HTTPS のみを使用する。
  - Gatekeeperの用語について
    - Gatekeeperは、OPAと同じ用語を多く採用している。そのため、Gatekeeperの動作を理解するためには、この用語がどのようなものであるかを知るが重要である。
      - Constraint
        - Constraintが定義されている場合、効果的に「これを許可しない」と表明していることになる→つまり、デフォルトは暗黙の許可
        - Kubernetesのリソース仕様は常に変化するため、このアーキテクチャ上の決定はKubernetesのリソース仕様にうまく適合している（whitelistだったらバージョンアップで死）
      - Rego
        - RegoはOPAネイティブのクエリ言語
      - Constraint Template
        - ポリシーのテンプレートと考えて差し支えない
        - 型付けされたパラメータと、再利用のためにパラメータ化された対象のRegoで構成される
    - 実際の例については省略
- 監査
  - すでにリソースがデプロイされているクラスタで、何が定義されたポリシーに準拠しているかを知りたい場合、どのように対処すればよいか
  - 監査機能を使用すると、Gatekeeperは定期的にリソースを定義された制約に照らして評価できる
  - ポリシーに従って設定されていないリソースを検出し、修正することができる
  - 監査結果は制約のステータスフィールドに格納されるため、`kubectl`を使用するだけで簡単に見つけることができる
- Gatekeeperを使いこなすために
  - Gatekeeperのリポジトリには、銀行のコンプライアンスに対応したポリシーを構築するための詳細な例を説明する素晴らしいデモコンテンツが同梱されている。
  - これをみながらやると良い
- Gatekeeper Next Steps(この中ですでに解決されているものもあるかも。2年前の本なので)
  - Mutation (ポリシーに基づいたリソースの変更。例えば、このようなラベルを追加する)  
  - 外部データソース（LDAPやActive Directoryとの統合によるポリシー検索）
  - 認可（Kubernetesの認可モジュールとしてGatekeeperを使用する。）  
  - ドライラン（クラスタでアクティブにする前に、ユーザがポリシーをテストできるようにする）
- ポリシーとガバナンスのベストプラクティス
  - Podの特定のフィールドを強制したい場合、どのKubernetesリソース仕様を検査し、強制したいかを決定する必要がある。
    - 例えば、Deploymentsの場合
      - DeploymentsはReplicaSetsを管理し、ReplicaSetsはPodを管理する
      - 3つのレベルすべてで実施することもできるが、この場合はポッドであるものが最良の選択。（のようにみえる）
      - しかし非準拠のポッドをデプロイしようとしたときのユーザーフレンドリーなエラーメッセージは表示されなくなる。
      - これは、ユーザーが非準拠のリソースを作成しているのではなく、ReplicaSetが作成しているため。
      - この経験から、ユーザーはDeploymentに関連付けられた現在のReplicaSetに対してkubectl describeを実行することで、そのリソースが準拠していないことを判断する必要がある。
      - これは面倒に思える かもしれないが、ポッドセキュリティポリシーなど、他のKubernetesの機能と一貫した動作となっている。
  - Kubernetesリソースには、kind、namespace、labelSelectorという基準で制約をかけることができる。
    - 制約を適用したいリソースに対して、できるだけタイトにスコープを設定することを強くお勧めする。
    - クラスタ上のリソースが大きくなっても一貫したポリシー動作が保証され、評価する必要のないリソースがOPAに渡されない
  - KubernetesのSecretなど、潜在的にセンシティブなデータに対して同期やエンフォースメントを行うことは推奨されない
    - OPAがこれをキャッシュに保持し（そのデータを複製するように設定されている場合）、リソースがGatekeeperに渡されることを考えると。。。
  - 多くの制約を定義している場合、制約の拒否はリクエスト全体の拒否を意味する。これを論理的なORとして機能させる方法はない。

## Chapter 12. Managing Multiple Clusters

- なぜ複数のクラスタが必要なのか
  - namespace, quota, RBAC, Pod Security Policy, Network Policyで制御はできるよね
  - 要因としては以下
    - ブラスト半径（いわゆる障害影響の大きさ）
      - マイクロサービスアークテクチャとして、サーキットブレーカー、リトライ、バルクッヘッド、レーン制限とかやるよね？（[Microsoftのやつがわかりやすい](https://learn.microsoft.com/ja-jp/azure/architecture/patterns/)）
      - インフラでも同じ。1つのアプリケーション起因で全てのサービスが死ぬことがある。
      - [本番のKubernetes環境における実際のカスケード障害について素晴らしい記事がある](https://danveloper.medium.com/on-infrastructure-at-scale-a-cascading-failure-of-distributed-systems-7cff2a3cd2df)
    - コンプライアンス（統制レベルの違い）
      - 例えばPCI（Payment Card Industry）やHIPAA（Health Insurance Portability and Accountability）などのワークロードには特別な考慮事項がある。
      - これらの準拠ワークロードは、セキュリティハードニング、非共有コンポーネント、または専用ワークロードの要件に関して、特定の要件を持っているかもしれない。
      - 特殊な方法で管理するよりも、クラスターを分けてしまった方が楽
    - セキュリティ（統制レベルの違い）
      - 大規模なKubernetesクラスタのセキュリティは、管理が難しくなる可能性がある。
      - 各チームが異なるセキュリティ要件を持つようになり、大規模なマルチテナントクラスタでそれらのニーズを満たすことが非常に困難になる可能性がある。
      - RBAC、ネットワークポリシー、ポッドセキュリティポリシーのネットワーク・ポリシーの小さな変更によって、不注意にもクラスタの他のユーザーにセキュリティ・リスクが及ぶ可能性がある。
      - クラスターを分けておけばセキュリティに対しての設定ミスはなくなる。
    - ハードなマルチテナンシー
      - クラスタ内で動作するすべてのワークロードと同じAPI境界を共有するため、ハードなマルチテナンシーを提供するわけではない。→namespaceはソフトなマルチてナンシ
      - 例えば敵対するソフトウェア同士を管理とかは難しいことがあるかもね（小さい組織だとなさそう）
    - 地域ベースのワークロード（マルチリージョン）
      - ディザスタリカバリや複数の地域で稼働する要件があれば
    - 特殊なワークロード（HPC, ML）
- マルチクラスターデザインの懸念点
  - マルチクラスタ設計を躊躇させるもの
    - データレプリケーション
      - データのレプリケーションと一貫性は、地理的な地域や複数のクラスタにワークロードを展開する上で常に重要なポイント→レプリケーション戦略
      - 地域間で一貫性があることと、レイテンシについて認識しておくこと
    - サービス・ディスカバリー
      - マルチクラスタ同士のサービスディスカバリをやりたいよね
      - Istio、Linkerd、Cillium、Consulとかマルチクラスターを前提にしたOSSがある
    - ネットワーク・ルーティング
      - クラスタ内は簡単だよね（フラットなネットワークでNATもない）
      - クラスタ外の通信はかなり複雑になってくる
        - クラスタ間のingress/egressのルーティングを考えなければならない
      - レイテンシーも考えなければならない
    - 運用管理
      - マルチクラスタを管理する上で最も重要なことの1つは、適切な自動化プラクティスを導入して運用負荷を軽減すること。
      - クラスタを自動化する場合、インフラストラクチャの展開とクラスタへのアドオン機能の管理を考慮する必要がある
        - Terraformとかを使うと良いよ
      - モニタリング、ロギング、イングレス、セキュリティ、その他のツールなど、クラスタへのアドオンを一貫して管理できるようにする必要がある。
      - セキュリティも運用管理の重要な要素であり、クラスタ間でセキュリティ・ポリシー、RBAC、ネットワーク・ポリシーを維持できるようにする必要がある
    - 継続的デプロイメント
      - 複数のクラスタと継続的デリバリー (CD) では、単一の API エンドポイントではなく、複数の Kubernetes API エンドポイントを扱う必要がある。
- マルチクラスターデプロイメントの管理
  - マルチクラスターデプロイメントを管理する際の最初のステップとしては、TerraformのようなIaCツールを使ってデプロイメントをセットアップすることが挙げられる
  - 最も重要なのは、再現性のためにクラスタデプロイをソースコントロールできるツールを使用すること
  - 自動化は、環境内の複数のクラスターをうまく管理するための鍵。クラスタのデプロイと運用のすべての側面を自動化することを優先する。
  - Kubernetes Cluster API
    - Cluster APIはKubernetesのプロジェクトで、クラスタの作成、設定、管理に宣言型のKubernetesスタイルのAPIを導入するもの
    - コアなKubernetesの上に、オプションで追加的な機能を提供する。
    - Cluster APIは、共通のAPIを通じて宣言されたクラスタレベルの設定を提供し、これにより、クラスタの自動化を中心としたツールの構築を容易に行うことができるようになる。
    - [書籍だと開発中とあったけど。。](https://cluster-api.sigs.k8s.io/)
    - Cluster APIのリポジトリをちゃんと追っていきましょう
  - Deployment and Management Patterns (operator)
    - Infrastructure as Softwareのコンセプトの実装として導入された。
    - Kubernetesクラスタにおけるアプリケーションやサービスのデプロイを抽象化することができる
      - 例えばpromethes operatorを使えば、デプロイメント、サービス、イングレスなどのk8sオブジェクトやバージョン、パーシステンスなどのprometheusの設定などを簡単に管理することができる
      - つまりこれはkubernetes apiの拡張
    - Operatorパターンは、2つのコンセプトに基づいて構築されている
      - Custom resource definitions（カスタムリソース定義）
      - Custom controllers(カスタムコントローラー)
    - カスタムリソース定義 (CRDs)はKubernetes APIを拡張するためのオブジェクトで、自分で定義したAPIを元に拡張することができる（ServiceとかConfigMapとかを自分で作るイメージ）
    - カスタムコントローラーはKubernetesのコアコンセプトであるリソースとコントローラの上に構築されている。namespace、Deployment、pod、または独自のCRDなどのKubernetes APIオブジェクトからのイベントを監視することで、独自のロジックを構築することができる（DeploymentからReplicasetが作られるのはコントローラーがやっていること）
    - Operatorパターンを活用すると、マルチクラスタの運用ツールで実行する必要がある運用タスクに自動化を組み込むことができる。
      - 簡単なものだとバックアップ/リストア
      - [Elasticsearch](https://github.com/upmc-enterprises/elasticsearch-operator)の運用など
        - 高可用性デプロイメントのためのゾーン  
        - マスターノードとデータノードのボリュームサイズ  
        - クラスタのリサイジング  
        - Elasticsearchクラスタのバックアップのためのスナップショット  
- GitOps
  - GitOpsはWeaveworksの人々によって広められたもので、そのアイデアと基本は、彼らが本番でKubernetesを運用した経験に基づいている
  - GitOpsは、ソフトウェア開発ライフサイクルの概念を、運用に応用したもの。
  - GitOpsでは、Gitリポジトリが真実のソースとなり、設定されたGitリポジトリとクラスタが同期される。
    - 例えば、Kubernetes Deploymentのマニフェストを更新すると、その設定変更は自動的にクラスタの状態に反映される。
    - この方法を使用することで、一貫性のあるマルチクラスタの維持が容易になり、フリート間での構成のドリフトを回避することができる。
  - GitOpsを使えば、複数の環境に対してクラスタを宣言的に記述し、その状態を維持するためのドライブをかけることができる。
  - GitOpsの実践は、アプリケーション・デリバリーと運用の両方に適用できる
- マルチクラスタ管理ツール
  - 複数のクラスタを扱う場合、異なるクラスタを管理するために異なるコンテキストを設定する必要があるため、Kubectlを使用するとすぐに混乱することがある
  - 複数のクラスタを扱う際にすぐにインストールしたくなるのがkubectxとkubensというツールで、複数のコンテキストと名前空間を簡単に変更することができる。
  - 本格的なマルチクラスター管理ツールが必要な場合はより人気のあるツールを使うといい
    - Rancher
      - 複数のKubernetesクラスタを一元的に管理するユーザーインターフェース（UI）。
      - オンプレミス、クラウド、ホスト型のKubernetesセットアップにまたがるKubernetesクラスタを監視、管理、バックアップ、リストアする。
      - また、複数のクラスタに展開されたアプリケーションを制御するためのツールを備え、運用ツールも提供する。
    - KQueen
      - Kubernetesクラスタのプロビジョニングのためのマルチテナントのセルフサービスポータルを提供
      - 複数のKubernetesクラスタの監査、可視化、およびセキュリティに重点を置いている。
      - KQueenは、Mirantisの人々によって開発されたオープンソースプロジェクトです。
    - Gardener
      - Kubernetesのプリミティブを利用してエンドユーザーにKubernetes as a Serviceを提供する（マルチクラスタ管理に対して異なるアプローチ）
      - Gardenerは、主要なクラウドベンダーをすべてサポートしており、SAPの人々によって開発された。
      - このソリューションは、Kubernetes as a Serviceの提供を構築しているユーザー向け。
- Kubernetes Federation
  - CRDとカスタムコントローラの概念を中心に構築されており、新しいAPIでKubernetesを拡張することが可能(KubeFedと呼ばれる)
  - KubeFedは必ずしもマルチクラスタ管理ではなく、複数のクラスタにまたがる高可用性（HA）デプロイメントを提供する。
  - Kubernetes上でアプリケーションを配信するために、複数のクラスタを1つの管理エンドポイントにまとめることができる。
  - たとえば、複数のパブリッククラウド環境に存在するクラスタがある場合、これらのクラスタを単一のコントロールプレーンにまとめ、すべてのクラスタへのデプロイメントを管理して、アプリケーションの耐障害性を向上させることができる。
  - この記事の執筆時点(2020)では、以下のFederatedリソースがサポートされている
    - Namespaces
    - ConfigMaps
    - Secrets
    - Ingress
    - Services
    - Deployments
    - ReplicaSets
    - Horizontal Pod Autoscalers
    - DaemonSets
    - Jobs
  - Federationでは、すべてがすべてのクラスタにコピーされるわけではないことを理解することが重要
    - DeploymentsとReplicaSetsでは、レプリカの数を定義し、それをクラスタに分散させる。
    - Deploymentsではこれがデフォルトだが、設定を変更することも可能。
    - 一方、namespaceを作成した場合、そのnamespaceはクラスタスコープされ、各クラスタで作成されます。
    - Secrets、ConfigMap、DaemonSetsも同じように動作し、各クラスタにコピーダウンされる。
    - Ingressリソースも前述のオブジェクトとは異なり、サービスへの単一のエントリポイントを持つグローバルなマルチクラスターリソースを作成する。
  - [KubeFedユーザーガイドを参照する](https://github.com/kubernetes-sigs/kubefed/blob/master/docs/userguide.md)ことで、このテーマについてより詳しく学ぶことができる
- マルチクラスター管理のベストプラクティス
  - クラスタのブラスト半径を制限して、連鎖的な障害がアプリケーションに大きな影響を与えないようにする。
  - PCI、HIPPA、HiTrustなどの規制上の懸念がある場合は、これらのワークロードと一般的なワークロードの混在の複雑さを緩和するために、マルチクラスタの活用を考えてみる
  - ハードなマルチテナントがビジネス要件である場合、ワークロードは専用のクラスターにデプロイする必要がある。
  - アプリケーションに複数のリージョンが必要な場合は、グローバルロードバランサーを利用して、クラスタ間のトラフィックを管理する。
  - HPCなどの特殊なワークロードを個別のクラスタに分割して、ワークロードの特殊なニーズを確実に満たすことができる
  - 複数の地域のデータセンターにまたがるワークロードを導入する場合は、まず、ワークロードのデータレプリケーション戦略を確認する
    - 地域をまたぐ複数のクラスタは簡単だが、地域をまたぐデータのレプリケーションは複雑になるため、非同期および同期のワークロードを処理するための健全な戦略があることを確認する。
  - prometheus-operator や Elasticsearch operator のような Kubernetes オペレータを活用して、自動化された運用タスクを処理する
  - マルチクラスター戦略を設計する際には、クラスター間のサービス・ディスカバリーやネットワーキングをどのように行うかも検討する。HashiCorpのConsulやIstioのようなサービス・メッシュ・ツールは、クラスタ間のネットワーキングを支援することができる。
  - CD戦略は、地域間や複数のクラスタ間での複数のロールアウトを処理できることを確認してみる。
  - 複数のクラスタ運用コンポーネントを管理するためにGitOpsアプローチを利用することを検討し、フリート内のすべてのクラスタ間の一貫性を確保する。
    - GitOpsアプローチは、必ずしもすべての環境でうまくいくとは限らないが、少なくとも、マルチクラスタ環境の運用負担を軽減するために調査する必要がある。

## Chapter 13. Integrating External Services and Kubernetes

- 私たちが構築するサービスのほとんどは、それらが稼働しているKubernetesクラスターの外側に存在するシステムやサービスと相互作用する必要がある。
  - 仮想マシンや物理マシンで稼働しているレガシーインフラからアクセスされる新しいサービスを構築しているためかもしれない
  - 構築しているサービスが、オンプレミスのデータセンターの物理インフラで稼働している既存のデータベースや他のサービスにアクセスする必要がある場合もある
  - 相互接続が必要なサービスを持つ複数の異なるKubernetesクラスタがある場合もある
- Kubernetesへのサービスのインポート
  - Kubernetesと外部サービスを接続する最も一般的なパターンは、KubernetesサービスがKubernetesクラスタの外部に存在するサービスを消費しているもの。
    - 多くの場合、これはKubernetesが何らかの新しいアプリケーション開発やオンプレミスのデータベースのようなレガシーリソースのインターフェースに使用されているため。(レガシー→モダナイズの典型的なパターン)
    - このパターンは、クラウドネイティブサービスのインクリメンタルな開発において最も理にかなっていることが多い。（重要なデータはオンプレのままのケースや、単にGraphQLだけ用意したいなど）
    - ミドルウェアの迅速な開発と信頼性の高い継続的なデプロイによって、最小限のリスクで大きな俊敏性を実現できるため、このレイヤーをKubernetesに移行することは多くの場合、非常に大きな意味を持つ。
  - これを実現するためには、Kubernetes内からデータベースにアクセスできるようにする必要がある。
    - Kubernetesから外部サービスにアクセスできるようにするというタスクを考えたとき、最初の課題は、単純にネットワークを正しく動作させること。
    - ネットワーキングの具体的な運用方法は、データベースの場所だけでなくKubernetesクラスタの場所にも大きく依存するため、一般的にクラウド型のKubernetesプロバイダでは、ユーザーが提供する仮想ネットワーク（VNET）にクラスタを展開し、その仮想ネットワークをオンプレミスネットワークでピアリングして接続できるようにすることが可能。
    - Kubernetesクラスタ内のポッドとオンプレミスのリソースとのネットワーク接続を確立したら、次の課題は外部サービスをKubernetesのサービスのように見せること。
    - Kubernetesでは、サービスの発見はDNS（Domain Name System）ルックアップを介して行われるため、外部データベースをKubernetesのネイティブな一部であるかのように感じさせるには、データベースを同じDNSで発見できるようにする必要がある。
  - セレクタレスサービスで安定したIPアドレスを実現する
    - Kubernetesクラスタと統合しようとしている外部リソースが安定したIPアドレスを持っていることを想定
    - セレクタなしのKubernetes Serviceを作成すると、そのサービスにマッチするPodが存在しないため、ロードバランシングが行われない。
    - その代わり、このセレクタレスサービスに、Kubernetesクラスタに追加したい外部リソースの特定のIPアドレスを持たせるようにプログラムすることができる。
    - そうすれば、Kubernetesポッドがyour-databaseのルックアップを実行したときに、内蔵のKubernetes DNSサーバーがそれを外部サービスのサービスIPアドレスに変換してくれる。

      ```yaml
      apiVersion: v1
      kind: Service
      metadata:
        name: my-external-database
      spec:
       ports:
         - protocol: TCP
           port: 3306
           targetPort: 3306
      ```

    - サービスが存在する場合、24.1.2.3でサービスを提供するデータベースIPアドレスを含むように、そのエンドポイントを更新する必要がある

      ```yaml
      apiVersion: v1
      kind: Endpoints
      metadata:
        # Important! This name has to match the Service.
        name: my-external-database
      subsets:
        - addresses:
            - ip: 24.1.2.3
          ports:
            - port: 3306
      ```
  
  - CNAMEベースのサービスによる安定したDNS名
    - Kubernetesクラスタと統合しようとしている外部リソースが安定したIPアドレスを持っていないことを想定（クラウド環境でIP固定していない場合とか）
    - DNSベースのロードバランサーの背後に複数のレプリカを配置しているサービスもある
    - このような状況では、クラスタに接続しようとしている外部サービスは安定したIPアドレスを持っていないが、安定したDNS名を持っている
    - 例えば、`database.myco.com`というDNS名を持つ外部データベースがある場合、myco-databaseという名前のCNAME Serviceを作成することができる。そのようなServiceは次のようになる。

      ```yaml
      kind: Service
      apiVersion: v1
      metadata:
        name: myco-database
      spec:
        type: ExternalName
        externalName: database.myco.com
      ```

    - CNAMEレコードは、安定したDNS名を持つ外部サービスをクラスタ内で発見可能な名前にマッピングするのに便利な方法。
    - 最初は、よく知られたDNSアドレスをクラスタローカルのDNSアドレスにマッピングし直すのは直感に反するように思えるかもしれないが、すべてのサービスが同じように見え、感じられるという一貫性は、通常は少量の複雑さを追加することに値すると言える。
    - さらに、CNAMEサービスは、すべてのKubernetesサービスと同様に、namespaceとに定義されるため、namespaceを使用して、同じサービス名（データベースなど）を、Kubernetesネームスペースに応じて異なる外部サービス（カナリアまたはプロダクションなど）にマッピングすることが可能。
  - アクティブコントローラベースのアプローチ
    - 限られた状況下では、Kubernetes内で外部サービスを公開するためのこれまでの方法のいずれも実行不可能。
      - Kubernetesクラスター内で公開したいサービスに対して、安定したDNSアドレスも単一の安定したIPアドレスも存在しない
    - このような状況では、Kubernetesクラスタ内で外部サービスを公開することは、かなり複雑になるが、不可能ではない。これを実現するためには、Kubernetes Servicesがその内部でどのように動作しているかをある程度理解しておく必要がある。
      - Kubernetes Servicesは、実際には2つの異なるリソースで構成されている
        - Serviceリソース
        - サービスを構成するIPアドレスを表すEndpointsリソース
      - 通常の運用では、Kubernetesのコントローラマネージャは、サービス内のセレクタを元にサービスのエンドポイントを投入している。
      - しかし、最初のstable-IPのアプローチのようにセレクタのないサービスを作成すると、選択されるポッドがないため、サービスのEndpointsリソースはポピュレートされない。
      - この状況では、正しいEndpointsリソースを作成し、投入するための制御ループを提供する必要がある
      - インフラストラクチャに動的に照会して、統合したいKubernetesの外部サービスのIPアドレスを取得し、そのIPアドレスでサービスのEndpointsを投入する必要がある。
      - これを実行すると、Kubernetesのメカニズムが引き継ぎ、DNSサーバーとkube-proxyの両方を正しくプログラムして、外部サービスへのトラフィックを負荷分散させる
- Kubernetesからサービスをエクスポート
  - Kubernetesから既存の環境にサービスをエクスポートする必要がある場合もある
    - 例えば、顧客管理用のレガシーな内部アプリケーションがあり、クラウドネイティブインフラで開発している何らかの新しいAPIにアクセスする必要があるために発生する可能性がある
    - あるいは、マイクロサービスベースの新しいAPIを構築しているが、社内ポリシーや規制上の要件から、既存の従来型Webアプリケーションファイアウォール（WAF）とのインタフェースが必要な場合もある
  - これが困難となる主な理由は、多くのKubernetesインストールにおいて、ポッドのIPアドレスがクラスターの外側からルーティング可能なアドレスでないため。
  - 従来のアプリケーションとKubernetesポッド間のルーティングを設定することが、Kubernetesベースのサービスのエクスポートを可能にするための重要なタスク
  - Internal Load Balancerを利用したサービスのエクスポート
    - Kubernetesからエクスポートする最も簡単な方法は、組み込みのServiceオブジェクトを使用すること（ALB, NLBとか）
    - これにより、クラスタの外側の仮想ネットワーク上で見える、安定したルート可能なIPアドレスを受け取ることができる。
    - その後、そのIPアドレスを直接使用するか、内部DNS解決をセットアップして、エクスポートされたサービスの検出を提供することができる
  - NodePortsでサービスをエクスポートする
    - オンプレだったらあるけど、クラウド利用だとあまりないかも。。
    - NodePortはクラスタに対して設定された範囲内でなければならない。この範囲のデフォルトは、30000から30999のポート。
  - 外部マシンとKubernetesの統合
    - Kubernetesサービスを外部のアプリケーションに公開するための最後の選択肢は、アプリケーションを実行しているマシンをKubernetesクラスタのサービス検出とネットワーキングのメカニズムに直接統合すること
    - 複雑なのであまりやりたくはないけど。。
    - ネットワーク用に外部マシンをクラスターに統合する場合、ポッドのネットワークルーティングとDNSベースのサービスディスカバリーが両方とも正しく動作することを確認する必要がある
    - 実施する上での簡単な方法はクラスタに参加させたいマシン上、クラスタ内のスケジューリングを無効にすること
      - `kubectl cordon...`でスケジューリングを無効にする
    - 他にも色々あるけど難しいので割愛（あまりやらなそうだし。。）
- Kubernetes間のサービス共有
  - もう1つの重要なユースケースは、Kubernetesクラスタ間でサービスを接続すること
    - 異なる地域のKubernetesクラスタ間で東西のフェイルオーバーを実現するため
    - 異なるチームが運営するサービス同士を連携させるため
  - これを実現するプロセスは、実際には前のセクションで説明した設計の組み合わせになる
    - まず、最初のKubernetesクラスター内でServiceを公開し、ネットワークトラフィックを流せるようにする。（IPアドレスができるよね）
    - 次に、この仮想IPアドレスを2番目のKubernetesクラスターに統合して、サービス・ディスカバリーを可能にする必要がある。これは、外部アプリケーションをKubernetesにインポートするのと同じ方法で実現する（最初のセクション）。
    - 最後にセレクタレスのServiceを作成し、最初に作成されたIPを登録する。
    - これらのステップはかなり手動であり、小規模で静的なサービスセットでは許容できるかもしれない
  - クラスタ間でより緊密な、または自動的なサービス統合を有効にしたい場合は、統合を実行するために両方のクラスタで実行するクラスタデーモンを書くことが理にかなっている
    - 特定のアノテーションを持つサービスを最初のクラスターで監視し、このアノテーションを持つすべてのサービスを、セレクタレスサービスを介して2番目のクラスターにインポートする
    - 同様に、同じデーモンが、第2クラスターにエクスポートされたものの、第1クラスターに存在しなくなったサービスを収集し、削除する
- 3rdパーティツール
  - ubernetesと任意のアプリケーションやマシンの両方でサービスを相互接続するために使用できるサードパーティのツールやプロジェクトがいろいろとある
    - 機能を提供してくれるけど、運用が複雑になることがあるから注意
  - 運用をサポートしてくれる商用もあるので検討しよう。
- クラスタと外部サービスの接続におけるベストプラクティス
  - クラスターとオンプレミス間のネットワーク接続を確立する。
    - ネットワークは、異なるサイト、クラウド、およびクラスター構成間で変化させることができるが、まず、ポッドがオンプレミスのマシンと会話できること、およびその逆が可能であることを確認する。
  - クラスター外のサービスにアクセスするには、セレクタレスサービスを使用し、通信したいマシン（データベースなど）のIPアドレスを直接プログラムすることができる。
    - 固定IPアドレスがない場合は、代わりにCNAMEサービスを使用してDNS名にリダイレクトすることができる。
    - DNS名も固定サービスもない場合は、外部サービスのIPアドレスをKubernetes Serviceのエンドポイントと定期的に同期させる動的なオペレータを書く必要があるかもしれない。
  - Kubernetesからサービスをエクスポートするには、内部ロードバランサーまたはNodePortサービスを使用する。
    - 内部ロードバランサーは、Kubernetes Service自体にバインドできる
    - パブリッククラウド環境では一般的にロードバランサーは使いやすいと思われる。そのようなロードバランサが利用できない場合、NodePortサービスはクラスタ内のすべてのマシンでサービスを公開することができる。
  - これら2つのアプローチを組み合わせて、サービスを外部に公開し、それをもう一方のKubernetesクラスタでセレクタレスサービスとして消費することで、Kubernetesクラスタ間の接続を実現することができる

## Chapter 14. Running Machine Learning in Kubernetes

- ちょっと飛ばす

## Chapter 15. Building Higher-Level Application Patterns on Top of Kubernetes

- Kubernetesは複雑なシステム。分散アプリケーションのデプロイと運用を簡素化するが、そのようなシステムの開発を容易にすることはできない
  - Kubernetesの上でより開発者に優しい基本要素を提供するために、より高いレベルの抽象化を開発することが理にかなっている。
  - アプリケーションの設定とデプロイの方法を標準化し、誰もが同じ運用のベストプラクティスに準拠できるようにすることが理にかなっている。
  - 開発者が自動的にこれらの原則に従うように、より高いレベルの抽象化機能を開発することによっても実現できる。しかし、このような抽象化を行うと、重要な詳細が開発者から隠され、特定のアプリケーションの開発や既存のソリューションの統合を制限したり、複雑にしたりする「壁」のような存在になる可能性がある。
  - より高度な抽象化機能を適切に設計することで、この分断を解消する理想的な道を歩むことができる。
- 高位抽象化機能を開発するためのアプローチ
  - Kubernetesの上でどのように上位の基本要素を開発するかを考えるとき、基本的なアプローチは2つある
    - Kubernetesを実装のディテールとして包むこと
      - プラットフォームを利用する開発者は、自分たちがKubernetesの上で動いていることをほとんど意識しないようにさせる
      - 開発者は提供するプラットフォームの消費者であると考える
    - Kubernetes自体に内蔵されている拡張性機能を利用する
      - Kubernetes Server APIはかなり柔軟で、Kubernetes API自体に任意の新しいリソースを動的に追加することができる
      - このアプローチでは、新しい上位のリソースはKubernetesの組み込みオブジェクトと共存し、ユーザーは組み込みと拡張の両方を含むすべてのKubernetesリソースと対話するために組み込みのツールを使用する
      - この拡張モデルは、Kubernetesが開発者にとってフロントかつ中心でありながら、複雑さを軽減して使いやすくするための追加機能を備えた環境となる
  - この2つのアプローチから、どのように適切なものを選べばいいのか?→構築する抽象化レイヤーの目標に大きく依存
    - 完全に分離された統合環境を構築し、ユーザーが「ガラスを割って」脱出する必要がないという強い確信があり、使いやすさが重要な特性である場合は、「Kubernetesを実装のディテールとして包む」やりかたが最適
      - 機械学習パイプラインを構築するとか（データサイエンティストにKubernetesを意識させない）
    - より高いレベルの開発者向け抽象化機能、たとえばJavaアプリケーションを簡単にデプロイする方法を構築する場合は「Kubernetes自体に内蔵されている拡張性機能を利用する」やり方が最適
      - アプリケーション開発の領域が非常に広いため、プリケーションとビジネスが時間とともに反復し、変化していく中で、開発者の要件やユースケースをすべて予測したものを作るのはおそらく無理
      - ツールのKubernetesエコシステムを引き続き活用できるようにすることで、新しいツールが開発されたりしても開発者にとって使いやすく提供できる
- Kubernetesの拡張
  - Kubernetesクラスターの拡張
    - Kubernetesクラスタを拡張するための完全なハウツーは大きなトピック
      - [Managing Kubernetes](https://www.oreilly.com/library/view/managing-kubernetes/9781492033905/)や[Kubernetes: Up and Running](https://www.oreilly.com/library/view/kubernetes-up-and/9781492046523/)を読むといい
      - 上記の書籍よりは詳しくは説明しない。Kubernetesの拡張性の使用方法について理解することに重点を置いて説明する。
    - Kubernetesクラスターを拡張するには、Kubernetesのリソースの接点を理解する必要がある。関連する技術的な解決策は3つある
      - サイドカー
        - これは、メインアプリケーションコンテナと並行して実行されるコンテナで、メインアプリケーションから切り離された追加機能を提供し、多くの場合、別のチームによって保守される。（別のチームによって保守されるのか。。）
        - サービスメッシュでは、サイドカーがコンテナ化されたアプリケーションに透過的な相互トランスポート・レイヤー・セキュリティ（mTLS）認証を提供する
        - しかし、開発者がサイドカーについて学び、使い方を知ることを求めると、実際には問題を悪化させることもある
      - Kubernetesを拡張するためのツールが追加され、物事を単純化することができる→アドミッションコントローラー
        - アドミッションコントローラーはKubernetes APIリクエストがクラスターのバッキングストアに保存される（または「アドミッション」）前に読み取るインターセプターである
        - APIオブジェクトを検証または変更することができる
        - 開発者がその利点を享受するためにサイドカーを知る必要がないように、クラスタで作成されたすべてのポッドにサイドカーを自動的に追加するために使用することができる
        - 開発者がKubernetesに提出したオブジェクトを検証するために使用することもできる(Gatekeeperもそう)
          - 例えば、開発者がKubernetesを使用するためのベストプラクティスに従ったPodやその他のリソースを送信することを保証するKubernetes用のリンターを実装することができる
      - 開発者にありがちなミスは、自分のアプリケーションのためにリソースを予約しないこと→リンターが使える
        - アドミッション・コントローラー・ベースのリンターは、そのようなリクエストを傍受し、拒否することができる
        - 上級ユーザが適宜リントルールをオプトアウトできるように、エスケープハッチ（例えば、特別なアノテーション）を残しておくことも必要
    - より高いレベルの抽象化を追加する方法としてカスタムリソース定義（CRD）がある
      - CRDは、既存のKubernetesクラスタに新しいリソースを動的に追加するための方法
    - Extending the Kubernetes User Experience
      - Kubernetesのユーザーエクスペリエンス（UX）も拡張することが有用な場合が多くある
      - kubectlプラグインをつくる
      - kubectlプラグインを利用することで、クラスタに追加した新しいリソースを深く理解するための新しいエクスペリエンスを定義することができる
- プラットフォーム構築時の設計上の考慮点
  - 開発者の生産性を向上させるために、これまで数え切れないほどのプラットフォームが構築されてきた
    - これらのプラットフォームが成功した場所と失敗した場所をすべて観察する機会があれば、他の人の経験から学ぶために、共通のパターンと考慮事項を開発することができる（本や記事を読みましょう。）
    - これらの設計ガイドラインに従うことで、構築したプラットフォームが、いずれ離れなければならない「レガシー」な行き止まりではなく、成功するプラットフォームであることを保証することができる
  - コンテナイメージへのエクスポートのサポート
    - プラットフォームを構築する際、多くの設計では、完全なコンテナイメージではなく、コード（例：Function as a Service [FaaS]の関数）またはネイティブパッケージ（例：JavaのJARファイル）を提供するだけでよいというシンプルさを実現している。
      - ユーザーがよく理解しているツールや開発経験の範囲内に留まることができるため、非常に魅力的である
      - アプリケーションのコンテナ化は、プラットフォームが代わりに行ってくれる
      - この方法の問題は、開発者が与えられたプログラミング環境の限界に直面したときに発生する
        - バグを回避するために特定のバージョンの言語ランタイムが必要な場合
        - アプリケーションの自動コンテナ化には含まれていない、追加のリソースや実行ファイルをパッケージ化する必要があるかも
    - プラットフォームのプログラミング環境を汎用コンテナにエクスポートすることをサポートしていれば、プラットフォームを利用する開発者は、ゼロから始めて、コンテナについて知っておくべきことをすべて学ぶ必要はない
    - その代わり、開発者は現在のアプリケーションを表す完全で実用的なコンテナイメージ（例えば、関数とノードランタイムを含むコンテナイメージ）を手に入れることができる
    - この出発点があれば、コンテナ・イメージを自分たちのニーズに合わせるために必要な微調整を行うことができる
    - このような漸進的な学習により、上位のプラットフォームから下位のインフラへの移行が劇的にスムーズになり、開発者に急な崖をもたらすことがないため、プラットフォームの一般的な有用性が高まる
  - サービスおよびサービスディスカバリーのための既存のメカニズムのサポート
    - プラットフォームに関するもう一つの共通点は、プラットフォームが進化し、他のシステムと相互接続すること
    - 多くの開発者はあなたのプラットフォームでとても幸せで生産的かもしれないが、どんな実世界のアプリケーションでも、あなたが構築したプラットフォームと低レベルのKubernetesアプリケーション、そして他のプラットフォームの両方にまたがることになる
    - Kubernetes用に構築されたレガシーデータベースやオープンソースアプリケーションへの接続は、十分に大きなアプリケーションの一部となるのが常である。
    - このような相互接続性の必要性から、サービスとサービス・ディスカバリーのためのコアKubernetes基本要素が、構築するどのプラットフォームでも使用され、公開されることが重要
    - プラットフォームのエクスペリエンスを向上させるために車輪の再発明をしないこと
    - プラットフォームで定義されたアプリケーションをKubernetesサービスとして公開すれば、クラスタ内のどのアプリケーションも、それらが上位のプラットフォームで実行されているかどうかに関係なく、アプリケーションを利用できるようになる
    - 同様に、Kubernetes DNSサーバーをサービス検出に使用すれば、上位のアプリケーションプラットフォームから、クラスタ内で動作している他のアプリケーションが上位のプラットフォームで定義されていない場合でも、接続することができるようになる
    - より良いもの、より使いやすいものを作りたいと思うかもしれないが、異なるプラットフォーム間での相互接続性は、十分な年齢と複雑さを持つアプリケーションに共通するデザインパターンである
    - 壁に囲まれた庭を作るという決断は、必ず後悔することになる
- アプリケーションプラットフォームの構築 ベストプラクティス
  - アドミッションコントローラーを使用して、クラスタへのAPIコールを制限および変更する
    - アドミッション・コントローラは、Kubernetesリソースを検証（および無効なリソースを拒否）することができる
    - ミューティングアドミッションコントローラは、新しいサイドカーやユーザーが知る必要のない他の変更を追加するために、APIリソースを自動的に変更することができる
  - kubectlプラグインを使用して、使い慣れた既存のコマンドラインツールに新しいツールを追加することで、Kubernetesのユーザーエクスペリエンスを拡張する。まれに、専用ツールの方が適切な場合がある。
  - Kubernetesの上にプラットフォームを構築する場合、そのプラットフォームのユーザーとそのニーズがどのように進化していくかを慎重に考える
    - シンプルで使いやすくすることは明らかに良い目標ですが、これが、プラットフォームの外側をすべて書き換えないと成功できないような、窮屈なユーザーを生み出すことになれば、最終的にはフラストレーションのたまる（そして成功しない）体験となる

## Chapter 16. Managing State and Stateful Applications

- コンテナオーケストレーションの初期は対象となるワークロードはステートレスアプリケーションだったが、最近はステートフルなアプリケーションも必要性を増している
- ボリュームとボリューム・マウント
  - 状態を維持する方法を必要とするすべてのワークロードが、複雑なデータベースや高スループットのデータ・キュー・サービスである必要はない
  - 多くの場合、コンテナ型ワークロードに移行するアプリケーションは、特定のディレクトリが存在し、それらのディレクトリに対して適切な情報を読み書きすることを期待する
  - このセクションでは、書き込み可能で、コンテナ障害や、さらに良いことにポッド障害にも耐えられるボリュームをコンテナに与えることに焦点を当てる
  - Docker、rkt、CRI-O、そしてSingularityなどの主要なコンテナランタイムでは、外部ストレージシステムにマッピングされたボリュームをコンテナにマウントすることが可能。
    - 最も単純に言えば、外部ストレージはメモリロケーション、コンテナのホスト上のパス、あるいはNFS、Glusterfs、CIFS、Cephなどの外部ファイルシステムになる
    - レガシーアプリケーションで、アプリケーション固有の情報をローカルファイルシステムに記録するように書かれているような場合に便利。
      - アプリケーションコードを更新して、サイドカーコンテナの標準出力または標準エラー出力にログアウトし、共有ポッドボリュームを介して外部ソースにログデータをストリームする
      - あるいはホストログとコンテナアプリケーションログの両方のボリュームを読み込めるホストベースのログツールを使用する
  - 以下に示すように、Kubernetes hostPathマウントを使用してコンテナ内のボリュームマウントを使用することで達成できる

    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
    name: nginx-webserver
    spec:
    replicas: 3
    selector:
      matchLabels:
        app: nginx-webserver
    template:
      metadata:
        labels:
          app: nginx-webserver
      spec:
        containers:
          - name: nginx-webserver
            image: nginx:alpine
            ports:
              - containerPort: 80
            volumeMounts:
              - name: hostvol
                mountPath: /usr/share/nginx/html
        volumes:
          - name: hostvol
            hostPath:
              path: /home/webcontent
    ```

    - ボリュームのベストプラクティス
      - ボリュームの使用は、データを共有する必要がある複数のコンテナを必要とするポッド（たとえば、アダプタまたはアンバサダー型のパターン）に限定するようにする。このようなタイプの共有パターンには、emptyDirを使用する。
      - ノードベースのエージェントやサービスからデータにアクセスする必要がある場合は、hostDir を使用する
      - 重要なアプリケーションログやイベントをローカルディスクに書き込むサービスを特定し、可能であればそれらをstdoutまたはstderrに変更し、ボリュームマップを活用する代わりに、真のKubernetes認識ログ集約システムにログをストリームさせるようにする
- Kubernetesストレージ
  - 本当の鍵は、ボリュームマウントをバックアップするストレージをKubernetesで管理できるようにすること
    - これにより、Podが必要に応じて生きたり死んだりするような、よりダイナミックなシナリオが可能になり、Podをバックアップするストレージは、Podがどこに住んでいてもそれに応じて移行するようにな
    - Kubernetesは、PersistentVolumeとPersistentVolumeClaimという2つの異なるAPIを使用して、Podのストレージを管理する
  - PersistentVolume
    - PersistentVolumeは、Podにマウントされているすべてのボリュームをバックアップするディスクと考える
    - Kubernetesは、動的または静的に定義されたボリュームのいずれかを使用できる。動的に作成されたボリュームを使用できるようにするには、KubernetesにStorageClassが定義されている必要がある
    - PersistentVolumeは様々なタイプとクラスでクラスタ内に作成でき、PersistentVolumeClaimがPersistentVolumeと一致した場合にのみ、実際にポッドに割り当てられる
    - ボリューム自体は、ボリューム・プラグインによってバックアップされる
    - Kubernetesで直接サポートされているプラグインは多数あり、それぞれ調整すべき設定パラメータが異なる

    ```yaml
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: pv001
      labels:
        tier: "silver"
      spec:
        capacity:
          storage: 5Gi
        accessModes:
          - ReadWriteMany
        persistentVolumeReclaimPolicy: Recycle
        storageClassName: nfs
        mountOptions:
          - hard
          - nfsvers=4.1
        nfs:
          path: /tmp
          server: 172.17.0.2
    ```

  - PersistentVolumeClaims
    - PersistentVolumeClaimsは、KubernetesにPodが使用するストレージのリソース要求定義を与える方法
    - Podはクレームを参照し、クレーム要求にマッチするpersistentVolumeが存在すれば、そのボリュームをその特定のPodに割り当てる
    - 最低限、ストレージ要求のサイズとアクセスモードが定義されている必要があるが、特定のStorageClassを定義することも可能
    - また、セレクタを使用して、特定の条件を満たす特定のPersistentVolumeが割り当てられるように一致させることもできる

    ```yaml
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
    name: my-pvc
    spec:
      storageClass: nfs
      accessModes:
        - ReadWriteMany
      resources:
        requests:
          storage: 5Gi
      selector:
        matchLabels:
        tier: "silver"
    ```

    - KubernetesはPersistentVolumeとclaimをマッチングし、それらをバインドする
    - このボリュームを使うには、pod.specは以下のようにクレームを名前で参照すればよい。

    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: nginx-webserver
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: nginx-webserver
      template:
        metadata:
          labels:
            app: nginx-webserver
        spec:
          containers:
            - name: nginx-webserver
              image: nginx:alpine
          ports:
            - containerPort: 80
          volumeMounts:
            - name: hostvol
              mountPath: /usr/share/nginx/html
          volumes:
            - name: hostvol
              persistentVolumeClaim:
                claimName: my-pvc
    ```

  - Storage Classes
    - 管理者は、事前に手動でPersistentVolumeを定義する代わりに、使用するボリュームプラグインと、そのクラスのすべてのPersistentVolumeが使用する特定のマウントオプションとパラメータを定義したStorageClassオブジェクトを作成することを選択することができる。
    - これにより、使用する特定のStorageClassでクレームを定義することができ、KubernetesはStorageClassパラメータとオプションに基づいて動的にPersistentVolumeを作成する

    ```yaml
    kind: StorageClass
    apiVersion: storage.k8s.io/v1
    metadata:
      name: nfs
    provisioner: cluster.local/nfs-client-provisioner
    parameters:
      archiveOnDelete: True
    ```

    - Kubernetesでは、オペレーターがDefaultStorageClassアドミッションプラグインを使用して、デフォルトのストレージクラスを作成することも可能
    - APIサーバーでこれが有効になっている場合、デフォルトのStorageClassを定義することができ、StorageClassを明示的に定義しないPersistentVolumeClaimsがある
    - クラウドプロバイダによっては、インスタンスで許可された最も安価なストレージにマッピングするために、デフォルトのストレージクラスが含まれている場合がある（大体アドオンを使うことになる）

    - コンテナ・ストレージ・インターフェース(CSI)とFlexVolume
      - 従来はFlexVolumeインターフェイスが使われてきたけど、CSIが標準になった
      - CSIの目的はストレージベンダー（SP）がプラグインを一度開発すれば、複数のコンテナオーケストレーション（CO）システムで動作するようにする業界標準のコンテナストレージインターフェイスを定義すること
  - Kubernetesストレージのベストプラクティス
    - クラウドネイティブアプリケーションの設計原則は、可能な限りステートレスアプリケーションの設計を強制しようとする
      - しかし、コンテナベースのサービスのフットプリントの増加により、データストレージの永続化の必要性が生じている
    - 可能であれば、DefaultStorageClassのアドミッションプラグインを有効にして、デフォルトのストレージクラスを定義する。
      - PersistentVolumesを必要とするアプリケーションのHelmチャートは、多くの場合、チャートのデフォルトストレージクラスをデフォルトとしており、アプリケーションをあまり修正することなくインストールすることができます。（Rookとかもそう）
    - オンプレミスまたはクラウドプロバイダーでクラスターのアーキテクチャを設計する場合、ノードとPersistentVolumesの両方に適切なラベルを使用して、計算層とデータ層の間のゾーンと接続性を考慮し、データとワークロードをできるだけ近くに保つためにアフィニティーを使用する。
      - ゾーンAのノード上のポッドが、ゾーンBのノードに接続されたボリュームをマウントしようとするのは、最も避けたいこと。（性能と障害ポイント）
    - どのワークロードがディスク上で状態を維持する必要があるか、慎重に検討する
      - データベースシステムのような外部サービスや、クラウドプロバイダで実行する場合は、現在使用されているAPIと一貫性のあるホスティングサービス、例えばmongoDBやmySQL as a serviceで処理できるか？
    - よりステートレスになるようにアプリケーションのコードを修正するのに、どれだけの労力がかかるかを判断する
    - Kubernetesはワークロードのスケジュールに合わせてボリュームを追跡してマウントするが、それらのボリュームに保存されるデータの冗長性とバックアップはまだ扱えない
      - CSI仕様では、ストレージバックエンドがサポートできる場合、ベンダーがネイティブスナップショットテクノロジーをプラグインするためのAPIが追加されている
    - ボリュームが保持するデータの適切なライフサイクルを検証する
      - デフォルトでは、動的プロビジョニングされたpersistentVolumeの再生ポリシーが設定されており、Podが削除されると、バックストレージ・プロバイダーからボリュームが削除される
      - 機密性の高いデータやフォレンジック分析に使用できるデータは、reclaimするように設定する必要がある
- ステートフル・アプリケーシ
  - まず、典型的なReplicaSetがどのようにPodをスケジュールおよび管理するか、そしてそれぞれが従来のステートフルアプリケーションにとってどのように有害になり得るかを理解する必要がある
    - ReplicaSetのPodは、スケジュールされるとスケールアウトされ、ランダムな名前が割り当てらる。
    - ReplicaSetのPodは、任意の方法でスケールダウンされる。 ReplicaSet内のPodは、名前またはIPアドレスを通じて直接呼び出されることはなく、Serviceとの関連付けを通じて呼び出される
    - ReplicaSet内のPodはいつでも再起動し、別のノードに移動することができる
    - PersistentVolumeがマッピングされたReplicaSet内のPodは、クレームによってのみリンクされるが、新しい名前を持つ新しいPodは、再スケジュールされたときに必要に応じてクレームを引き継ぐことができる
  - クラスタ・データ管理システムについて簡単な知識しかない人でも、ReplicaSetベースのポッドのこのような特性についてすぐに問題を理解することができる。
    - データベースの書き込み可能なコピーを持つポッドが突然削除されることを想像してみると。。。
    - データベースシステムがリーダーエレクションプロセスを必要とすること、セットのメンバー間のデータレプリケーションを処理できること、できないこと、あるいはそれどころか、それがデータベースシステムであることを全く知らない。そこで、ステートフルセットの出番となる
  - StatefulSets
    - StatefulSetが行うのは、より信頼性の高いノード/ポッドの動作を期待するアプリケーション・システムの実行を容易にすること
    - ReplicaSetの典型的なPodの特性のリストを見てみると、StatefulSetsはほとんど正反対のものを提供している
      - StatefulSet内のPodは、逆の順序でスケールダウンされる
      - StatefulSet内のPodは、ヘッドレス・サービスの背後で名前により個別にアドレス指定することができる
      - ボリューム・マウントを必要とするStatefulSet内のPodは、定義されたPersistentVolumeテンプレートを使用する必要がある。StatefulSetのPodが要求するボリュームは、StatefulSetが削除されても削除されない。
    - StatefulSet の仕様は、Service宣言とPersistentVolumeテンプレート以外はDeploymentと非常によく似ている。最初にヘッドレスServiceを作成する必要があり、これはPodが個別に対応するServiceを定義するものである。ヘッドレスServiceは、通常のServiceと同じだが、通常のロードバランシングは行わない。

    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
    name: mongo
    labels:
      name: mongo
    spec:
      ports:
      - port: 27017
        targetPort: 27017
      clusterIP: None #This creates the headless Service
      selector:
        role: mongo
    ```

    ```yaml
    apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      name: mongo
    spec:
      serviceName: "mongo"
      replicas: 3
      template:
        metadata:
          labels:
            role: mongo
            environment: test
        spec:
          terminationGracePeriodSeconds: 10
          containers:
            - name: mongo
              image: mongo:3.4
              command:
                - mongod
                - "--replSet"
                - rs0
                - "--bind_ip"
                - 0.0.0.0
                - "--smallfiles"
                - "--noprealloc"
              ports:
                - containerPort: 27017
              volumeMounts:
                - name: mongo-persistent-storage
                  mountPath: /data/db
            - name: mongo-sidecar
              image: cvallance/mongo-k8s-sidecar
              env:
                - name: MONGO_SIDECAR_POD_LABELS
                  value: "role=mongo,environment=test"
      volumeClaimTemplates:
        - metadata:
          name: mongo-persistent-storage
          annotations:
            volume.beta.kubernetes.io/storage-class: "fast"
          spec:
            accessModes: [ "ReadWriteOnce" ]
            resources:
              requests:
                storage: 2Gi
    ```

  - Operators
    - StatefulSetの現実的な問題はKubernetesはStatefulSetで実行されているワークロードを本当に理解していないこと。バックアップ、フェイルオーバー、リーダー登録、新規レプリカ登録、アップグレードなど、その他の複雑なオペレーションはすべて、かなり定期的に行われる必要があるオペレーションで、StatefulSetとして実行する場合は、ある程度慎重に検討する必要がある。→Operatorを使おう
    - Operatorはステートフルなアプリケーションのためだけでなく、カスタムコントローラロジックのため、複雑なデータサービスやステートフルなシステムにも間違いなく適応する
    - 複雑な分散システムをKubernetesで運用する際の運用知識を取り入れたいと考える世界中の多くのデータ管理システムベンダー、クラウドプロバイダ、SREの間で徐々に足場を固めつつある（この本は2020年出版）
    - [OperatorHub](https://operatorhub.io/)では、キュレーションされたOperatorの最新リストを見ることができます

  - StatefulSetとOperatorのベストプラクティス
    - 通常、ステートフルなアプリケーションは、オーケストレーターがまだうまく管理できない、
      - より深い管理を必要とするため、ステートフルセットを使用する決定は、慎重に行う必要がある
    - StatefulSetのヘッドレスServiceは自動的に作成されず、個々のノードとしてのPodを適切にアドレス指定するために、デプロイ時に作成する必要がある
    - アプリケーションが序数命名と信頼できるスケーリングを必要とする場合、それが常にPersistentVolumesの割り当てを必要とするとは限らない
    - クラスタのノードが応答しなくなった場合、StatefulSetの一部であるPodは自動的に削除されず、猶予期間後に TerminatingまたはUnkown状態になる
      - このポッドを消去する唯一の方法は、クラスタからノード・オブジェクトを削除するか、kubeletが再び動作してポッドを直接削除するか、またはOperatorがポッドを強制的に削除すること
      - 強制削除は最後のオプションであり、削除されたポッドを持つノードがオンラインに戻らないように十分に注意する必要がある。`kubectl delete pod nginx-0 --grace-period=0 --force` でPodを強制削除することができる。
    - 強制削除してもポッドがUnknownのままになっている場合があるので、APIサーバーにパッチを当てるとエントリーが削除され、StatefulSetコントローラーが削除したポッドの新しいインスタンスを作成する。`kubectl patch pod nginx-0 -p '{"metadata":{"finalizers":null}}'`
    - 何らかのリーダー選出やデータレプリケーションの確認処理を伴う複雑なデータシステムを運用している場合は、グレースフル・シャットダウン処理を用いてポッドを削除する前に、preStopフックを用いてあらゆる接続を適切に閉じ、リーダー選出を強制し、データ同期を確認する
    - ステートフル・データを必要とするアプリケーションが複雑なデータ管理システムである場合、アプリケーションのより複雑なライフサイクル・コンポーネントの管理を支援するOperatorが存在するかどうかを確認する価値があるかもしれない。
      - アプリケーションが社内で構築されている場合、アプリケーションに管理性を追加するために、アプリケーションをOperatorとしてパッケージ化することが有用であるかどうかを調査する価値があるかもしれない。
      - 例として、[CoreOS Operator SDK](https://sdk.operatorframework.io/)をみてみよう

## Chapter 17. Admission Control and Authorization

## Chapter 18. Conclusion
